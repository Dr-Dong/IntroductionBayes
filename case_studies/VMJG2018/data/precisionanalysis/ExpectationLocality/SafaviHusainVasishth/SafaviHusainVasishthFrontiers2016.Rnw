% !TeX program = pdfLaTeX

\documentclass{frontiersSCNS} % for Science articles

\usepackage{url,lineno}
\usepackage{graphicx}
\usepackage{epstopdf}

\linenumbers

\copyrightyear{}
\pubyear{}

\def\journal{Psychology}
\def\DOI{}
\def\articleType{Research Article}
\def\keyFont{\fontsize{8pt}{11pt}\helveticabold }
\def\firstAuthorLast{Safavi {et~al.}} %use et al only if is more than 1 author
\def\Authors{Safavi, Molood Sadat\,$^{1}$,  Husain, Samar\,$^{2}$, and Vasishth, Shravan\,$^{1*}$}

\def\Address{
$^{1}$Department of Linguistics, University of Potsdam, Potsdam, Germany \\
$^{2}$Department of Humanities and Social Sciences, Indian Institute of Technology, India
}

\def\corrAuthor{Shravan Vasishth}
\def\corrAddress{Department of Linguistics, University of Potsdam, Karl-Liebknecht-Str. 24-25, D-14476 Potsdam, Germany}
\def\corrEmail{vasishth@uni-potsdam.de}


\newcommand{\writenote}[1]{
{\fontfamily{iwona}\selectfont
\color{blue}
  {[}#1{]} }
}

\newcommand{\posstextcite}[1]{\citeauthor{#1}'s (\citeyear{#1})}


\usepackage{gb4e}
\noautomath

\begin{document}

<<setup,cache=FALSE,include=FALSE>>=
# global chunk options
opts_chunk$set(cache=TRUE, autodep=TRUE,fig.path='Figures/SafaviEtAlfigure', fig.align='center', fig.show='hold', cache.path='cache/graphics-',fig.height=5)
opts_knit$set(self.contained=FALSE)

#for labeling
knit_hooks$set(rexample = function(before, options, envir) {
  if (before) sprintf('\\begin{rexample}\\label{%s}\\hfill{}', options$label) 
    else '\\end{rexample}'
})
    

library(knitr)
require(lme4)
require(gridExtra)
@

\onecolumn
\firstpage{1}

\title[Dependency resolution difficulty in Persian complex predicates]{Dependency resolution difficulty increases with distance in Persian separable complex
predicates: Evidence for expectation and memory-based accounts}
\author[\firstAuthorLast ]{\Authors}
\address{}
\correspondance{}
\extraAuth{}
\topic{Encoding and navigating linguistic representations in memory}

\maketitle


\begin{abstract}
\section{}
   Delaying the appearance of a verb in a noun-verb dependency tends to increase processing difficulty at the verb; one explanation for this locality effect is decay and/or interference of the noun in working memory. Surprisal, an expectation-based account, predicts that delaying the appearance of a verb either renders it no more predictable or more predictable, leading respectively to a prediction of no effect of distance or a facilitation.  Recently, Husain et al (2014) suggested that when the exact identity of the upcoming verb is predictable (strong predictability), increasing argument-verb distance leads to facilitation effects, which is consistent with surprisal; but when the exact identity of the upcoming verb is not predictable (weak predictability), locality effects are seen. We investigated Husain et al.'s proposal using Persian complex predicates (CPs), which consist of a non-verbal element---a noun in the current study---and a verb. In CPs, once the noun has been read, the exact identity of the verb is highly predictable (strong predictability); this was confirmed using a sentence completion study. In two self-paced reading (SPR) and two eye-tracking (ET) experiments, we delayed the appearance of the verb by interposing a relative clause (Expt. 1 and 3) or a long PP (Expt. 2 and 4). 
We also included a simple Noun-Verb predicate configuration with the same distance manipulation; here, the exact identity of the verb was not predictable (weak predictability). Thus, the design crossed Predictability Strength and Distance.  We found that, consistent with surprisal, the verb in the strong predictability conditions was read faster than in the weak predictability conditions. Furthermore, greater verb-argument distance led to slower reading times;  strong predictability did not neutralize or attenuate the locality effects. As regards the effect of distance on dependency resolution difficulty, these four experiments  present evidence in favor of  working memory accounts of argument-verb dependency resolution, and against the surprisal-based expectation account of Levy (2008). However, another expectation-based measure, entropy, which was computed using the offline sentence completion data, predicts reading times in Experiment 1. We suggest that forgetting due to memory overload leads to greater entropy at the verb. 
\tiny
\keyFont{ 
\section{Keywords:} Locality, Expectation, Surprisal, Entropy, Persian, Complex Predicates, self-paced reading, eye-tracking} 
\end{abstract}


\section{Introduction}
A long-standing claim in sentence processing is that increasing distance in a linguistic dependency, such as a noun-verb dependency, leads to greater processing difficulty \citep{aspects,just1992ctc,gibson2000dependency, lewis2005activation}; it is common to  refer to this increase in processing difficulty as the locality effect. One explanation for the locality effect is in terms of constraints imposed by working memory. According to one account, the Dependency Locality Theory (DLT; \citealp{gibson1998linguistic}),  the processing difficulty experienced when resolving a long dependency depends on the decay experienced by the noun; a related account by \citet{lewis2005activation} attributes the locality effect to decay and/or interference. Constraints on working memory may be a plausible explanation 
given that individuals' working memory capacity seems to affect the processes involved in  dependency resolution  \citep{nicenboim2014individual,caplan2013memory}.
Although there is evidence consistent with the memory-based explanation in English, German, Chinese, Russian, and Hindi, \citep{hsiao03,grodner2005consequences,bartek2011search,vasishth2011locality,levy2013syntactic,husain2014strong,HusainVasishthNarayanan2015}, research on some of these languages has also uncovered evidence that increasing noun-verb distance facilitates processing at the verb \citep{konieczny2000locality,vasishth2003working,vasishth2006argument,jaegeretalCUNY08,vasishth2011locality,levy2013expectation,husain2014strong,Jaegeretal2015}. 
One explanation for these anti-locality effects is in terms of surprisal \citep{hale2001probabilistic,levy2008expectation}.
Surprisal extends and formalizes the old idea of predictive sentence processing---which has been extensively investigated in the EEG literature (e.g., \citealt{KutasHillyard1984})---in terms of probabilistic parse continuations (also see \citealt{jurafsky96}). 
The surprisal account assumes that the comprehender maintains and uses linguistic knowledge probabilistically to parse a sentence incrementally.  Surprisal is the claim that rare transitions are difficult: increased processing difficulty is predicted when a parser is required to build a low-probability syntactic structure. Formally, surprisal is defined as the negative log probability of encountering a particular part of speech or word given previous context. We will refer to surprisal as the expectation-based account, following the terminology of  \citet{levy2008expectation}.\footnote{Another expectation-based account in the literature is the entropy reduction hypothesis or ERH \citep{hale06}; we do not investigate ERH in this paper, but we do discuss a related idea, entropy, in the General Discussion.} 

In many of these studies, evidence has been found for both the memory-based accounts and the expectation-based account. One conclusion that has emerged is that both memory and expectation play a role.  For example, in his eye-tracking study investigating processing difference in English object vs subject relative clauses, \cite{staub2010eye} finds evidence for both expectation-based processing and locality constraints, although these occur in different regions of the target sentence.  An example of Staub's design is provided below. In this study, processing difficulty was found on the noun phrase \textit{the fireman} in the ORC (object relative clause) \ref{staubsentences}b, compared to the SRC (subject relative clause) \ref{staubsentences}a; this is consistent with the expectation account because the reader would be forced to build a rare object relative in the ORC condition when he/she encounters the noun phrase. However, this study also found greater processing difficulty at the relative clause verb in ORCs than SRCs, which is predicted by memory accounts. 
 
\begin{exe}
\ex \label{staubsentences}
\begin{xlist}
\item The employees that noticed the fireman hurried across the open field.\\
\item The employees that the fireman noticed hurried across the open field.\\
\end{xlist}
\end{exe}

As further examples, both  \cite{vasishth2011locality} and \cite{levy2013expectation} have argued that locality effects may appear when high working memory load is experienced; anti-locality effects may be present when the load is low.  

In a recent development, \cite{husain2014strong} argue that the strong predictability for a head (predicting an exact lexical item) can neutralize the locality effect; locality may manifest itself only when predictability strength is weak, that is, when only a verb phrase is predicted, and not the exact identity of the verb.  
  In their self-paced reading study, \cite{husain2014strong} used a 2$\times$2 design, crossing Predictability and Dependency Distance to investigate locality and anti-locality effects. In the strong predictability conditions,  Hindi complex predicates were used. In these noun-verb sequences, the noun strongly predicted the upcoming light verb, e.g., the noun \textit{khayaal}, `care', strongly predicts the verb \textit{rakhnaa}, `put', in \textit{khayaal rakhnaa}, literally, `care put' (`to take care of'). The weak predictability condition, on the other hand, used the same verb used in the complex predicate, but the noun did not predict the verb. An example is  \textit{gitaar rakhnaa}, `guitar put'; `to put (down) a guitar'; here, the verb retains its literal meaning. Thus, when the reader sees \textit{gitaar}, they cannot predict the exact identity of the verb, because many other verbs are possible here (e.g., bought). 
  To summarize, in the strong predictability condition, the noun predicted the exact identity of the verb, while in the weak predictability condition the exact identity of the verb was not predicted with high certainty---although \textit{a} verb was predicted.  The second factor, dependency distance, was manipulated by placing one to two adverbials between the nominal predicate/object and the verb in the short condition. The long condition had two to three intervening adverbials. Reading time was measured at the verb. The results showed that CP light verbs were read faster in long vs.\ short distance conditions, but for the non-CP verb there was a tendency towards a slowdown in long vs.\ short conditions. Finally, there was weak evidence for an interaction (estimate on the log ms scale: 0.03, Bayesian 95\% credible interval [-0.02, 0.07], posterior probability of the effect being greater than 0 was 0.77). That is, there was some indication that with increased distance there was a speedup at the light verb in the CP conditions and a slowdown in the non-CP conditions. Although these results can also be interpreted as showing no interaction,  Husain and colleagues suggested that strong predictability of the head could be canceling the locality effect, with the locality effect manifesting itself only when predictability strength was weak. 
  
In the present study, we build on the work by \cite{husain2014strong} described above. Husain and colleagues' work suggested that the strength of the predictability may modulate whether locality effects occur or not; we investigate the cross-linguistic generality of this claim using Persian, which, like Hindi, also has a complex predicate construction that allows us to manipulate strong and weak predictability. 
We turn next to a short discussion of the complex predicate construction in Persian as it relates to our experiments.

\section{Complex Predicates in Persian}

Complex Predicates (CPs)   consist of a sequence containing a non-verbal element (e.g., a  noun) and a verb, where the meaning of the sequence is non-compositional \citep{samvelian2001syntax}.  An example is shown in (\ref{exampleCP}).

\begin{exe}
\ex \label{exampleCP}
\gll Maryam be man latme zad \\
Maryam to me   damage hit\\
\glt 'Maryam caused damage to me (Maryam harmed me).'
\end{exe}

The verb, often called a light verb, lacks sufficient semantic force to function as an independent predicate \citep{vahedi1996syntax,karimi1997light,karimi2005light} and can be combined with different types of non-verbal items such as nominal, adjectivals or prepositional phrases \citep{dabir1997compound}. 

In our study, we used separable complex predicates as defined by \citet{karimi2011separability}. According to Karimi-Doostan, a complex predicate can be separated if it satisfies both of the following two conditions: (1) if the nominal part is a noun to which adjectives, demonstratives, and wh-words, etc. can be attributed, and (2) if this noun has an internal argument structure (referring to an action or event). From this perspective, Persian complex predicates are categorized in three groups: (1) predicative verbal nouns (e.g., \textit{anja:m da:dan}, perform+to give), (2) predicative nouns (e.g., \textit{latme zadan}, damage+to hit), and (3) non-predicative nouns (e.g., \textit{gush da:dan}, ear+to do). Among these three types, only the second one satisfies both of the conditions. 

We began by independently validating our assumption that the CPs we used in our experiments are predictable and separable.
We first conducted a norming study  (a sentence completion task), to establish that the light verbs (of the separated CPs) are highly predictable when the nominal is provided, as compared to non-CP verbs in simple predicate conditions. 
We then conducted an acceptability rating study to determine  how acceptable Persian CPs are when they get separated.  

\section{Norming studies}\label{normingstudies}

In order to prepare appropriate stimuli, two norming studies were run. 
The first study involved offline sentence completion and served to validate (i) whether the identity of the verb in the complex predicate is highly predictable, and (ii) whether the identity of the verb in the control conditions is not predictable.

The second study involved offline acceptability rating; the goal was to choose  complex predicates for our experiments which are separable. That is, we wanted to identify complex predicates which native speakers would find acceptable even if an intervener occurs between the noun-verb sequence.
Instructions for both studies are provided in the supplementary materials.

The sentence completion study was carried out to derive the predictions of the expectation account. Previous work on expectation effects suggests that sentence completion data may be useful for this purpose. For example, \citet{levy2013expectation} used sentence completion data to complement their corpus analyses for deriving their predictions. In their study, the key issue was whether the intervening material (e.g., a dative marked NP) leads to a prediction of a dative verb. Their Table 4 shows that the intervening material sharpened the expectation for the type of verb predicted. This shows that sentence completion data can be used to determine empirically whether the prediction for a specific verb or a verb type is sharpened by intervening material; in the Levy and Keller case, it makes sense that the intervener sharpens the expectation, but clearly the nature and content of the intervening phrases will be crucial in determining whether expectations are sharpened \citep{konieczny2000locality,grodner2005consequences}.\footnote{We return to this point in the General Discussion, where we discuss the effect of entropy on reading times.}
Similarly, \citet{husain2014strong} used sentence completion to establish that the identity of the verb in a complex predicate is highly predictable given the preceding context, but the identity of the verb in a simple predicate is not (see their Table~4). A third example is  
\citet{Jaegeretal2015}; they used both corpus data and sentence completion to establish that a sentence starting with a determiner, classifier, and an adverb leads to the prediction of a relative clause continuation in Chinese, and that the conditional probability of a subject relative continuation is higher than that of an object relative continuation (see their Table~2). 
Given these previous results, we assume that sentence completion data is informative about the predictions of the expectation-based account.

\subsection{Sentence completion studies}

Two groups (32 participants each) of Persian native speakers, who did not take part in any of the other experiments, participated in two sentence completion pre-tests in which they were asked to complete the sentences after they were presented the sentence fragment until the pre-critical word.  For example, as shown in example~\ref{examplecontinuations}, subjects were shown incomplete sentences which they had to complete; in this example, the missing verb is shown in parentheses. The participants were allowed to complete the sentence with as many words as they wanted, but our interest was only in the first word that they would write, which would most likely be a verb.
This allowed us to calculate the proportion of continuations in which the exact verb was produced.

\begin{exe}
\ex \label{examplecontinuations}
\begin{xlist}
\item 
\gll Ali a:rezouyee bara:ye man (kard) \dots \\
     Ali wish-INDEF for 1.S (do-PST \dots \\
\glt    `Ali (made) a wish for me \dots ' \\

\item 
\gll Ali a:rezouyee ke besya:r doost-da:sht-am bara:ye man (kard) \dots \\
      Ali wish-INDEF that {a lot} like-1.S-PST for 1.S (do-PST) \dots \\
\glt `Ali (made) a wish that I liked a lot for me \dots' \\
\end{xlist}
\end{exe}

<<loadlibraries,echo=FALSE,include=FALSE>>=
library(lme4)
library(RePsychLing)
library(car)
library(reshape)
library(ggplot2)
library(plyr)
library(grid)
library(lattice)
library(xtable)
library(rstanarm)
@


<<sentcomp,echo=FALSE>>=
## load sentence completion data:
d<-read.table('Pretests/entropy/sentcomp.txt', header=T)
## Expt 1:
exp1<-subset(d, Pre.test==1)
## three subjects incorrectly labeled twice with same ID:
## fix this:
## Fix problem by pasting list no. to make subjects unique:
exp1$subject<-factor(paste(exp1$subj.ID,exp1$LatSq.list,sep=""))
#unique(exp1$subject)

## sanity check:
#xtabs(~subject+cond,exp1)
#dim(subset(exp1,cond%in%c("a","b")))
#xtabs(~subject+item.ID,exp1)

exp2<-subset(d, Pre.test==2)
#xtabs(~subj.ID+cond,exp2)
exp2$subject<-factor(paste(exp2$subj.ID,exp2$LatSq.list,sep=""))
#unique(exp2$subject)

## compute accuracy of producing target verb:
## target verbs in sentence completion study,
## these are the verbs subjects should produce:
target_verbs_ab<-c("kardan","zadan",
                "kardan","kardan",
                "kardan","kardan",
                "kardan","dashtan",
                "bordan","kardan",
                "zadan","zadan",
                "zadan","zadan",
                "zadan","zadan",
                "kardan","kardan",
                "kardan","kardan",
                "dadan","kardan",
                "dadan","dadan",
                "goftan","kardan",
                "kardan","kardan",
                "gozashtan","zadan",
                "kardan","kardan",
                "kardan","kardan",
                "gereftan","kardan")
             
target_verbs_cd<-c("pokhtan", "kharidan", 
                   "dukhtan", "shenidan", 
                   "shenidan", "neveshtan", 
                   "khandan", "dashtan", 
                   "dashtan", "gereftan", 
                   "avikhtan", "goftan", 
                   "forukhtan", "dadan", 
                   "avardan", "dadan", 
                   "amukhtan", "fahmandan", 
                   "bakhshidan", "dadan", 
                   "resandan", "sepordan", 
                   "ferestadan", "dadan", 
                   "dadan", "khandan", 
                   "goftan", "navakhtan", 
                   "raftan", "shenidan", 
                   "shenidan", "fahmidan", 
                   "andishidan", "fahmidan", 
                   "fahmidan", "neveshtan")   

## data frame with target verbs:
targets_ab_df<-data.frame(item=rep(1:36,each=2),
                    cond=rep(letters[1:2],36),
                    target_verbs=rep(target_verbs_ab,each=2))
targets_cd_df<-data.frame(item=rep(1:36,each=2),
                    cond=rep(letters[3:4],36),
                    target_verbs=rep(target_verbs_cd,each=2))
targets<-rbind(targets_ab_df,targets_cd_df)

## Exp 1:
nrows<-dim(exp1)[1]
## Next, create a column marking "correct completion"
hit<-rep(NA,nrows)

## identify exact matches with target:
for(i in 1:nrows){
  ## get i-th row:
  tmp<-exp1[i,]
  itemid<-tmp$item.ID
  condition<-as.character(tmp$cond)
  tmpverb<-as.character(tmp$verb)
  ## find appropriate row in targets data frame:
  target_row<-subset(targets,item==itemid & 
                       cond==condition)
  targetverb<-as.character(target_row$target_verbs)
  if(targetverb==tmpverb){
    hit[i]<-1} else {hit[i]<-0}
  }

## 1 if target verb produced, 0 otherwise
exp1$target<-hit

## Exp 2:
## Next, create a column marking "correct completion"
nrows<-dim(exp2)[1]
hit<-rep(NA,nrows)

for(i in 1:nrows){
  ## get i-th row:
  tmp<-exp2[i,]
  itemid<-tmp$item.ID
  condition<-as.character(tmp$cond)
  tmpverb<-as.character(tmp$verb)
  ## find appropriate row in targets data frame:
  target_row<-subset(targets,item==itemid & 
                       cond==condition)
  targetverb<-as.character(target_row$target_verbs)
  if(targetverb==tmpverb){
    hit[i]<-1} else {hit[i]<-0}
  }

## 1 if target verb produced, 0 otherwise
exp2$target<-hit

## slight reduction in prob. of target with distance:
sentcomp_mn_exp1<-round(100*with(exp1,tapply(target,cond,mean)),digits=2)
sentcomp_mn_exp2<-round(100*with(exp2,tapply(target,cond,mean)),digits=2)

## contrast coding:
exp1$Predictability<-ifelse(exp1$cond%in%c("a","b"),1,-1)
exp1$Distance<-ifelse(exp1$cond%in%c("a","c"),-1,1)

exp2$Predictability<-ifelse(exp2$cond%in%c("a","b"),1,-1)
exp2$Distance<-ifelse(exp2$cond%in%c("a","c"),-1,1)

library(lme4)
## fails to converge:
#sentcomptarget<-glmer(target~predictability*distance+
#                 (1+predictability*distance||subject)+
#                 (1+predictability*distance||item.ID),
#               family=binomial(),
#               exp1)
#summary(sentcomptarget)

sentcomptargete1<-glmer(target~Distance*Predictability+
                 (1|subject)+
                 (1|item.ID),
               family=binomial(),
               exp1)

sentcomptargete2<-glmer(target~Distance*Predictability+
                 (1|subject)+
                 (1|item.ID),
               family=binomial(),
               exp2)

#summary(sentcomptargete1)
#summary(sentcomptargete2)

## Priors: plausible values of 
## prob range from 0.0001 to 0.9999
## Intercept: t(2) a bit like N(0,2.96^2)
## Slope: t(2) a bit like N(0,2.96^2)

library(rstanarm)

if(0){ ## avoiding running at compile time
sentcompstane1<-stan_glmer(target~Distance*Predictability+
                    (1+Distance*Predictability|subject)+
                    (1+Distance*Predictability|item.ID),
                  family=binomial(),
                  prior_intercept=student_t(df=2),
                  prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  exp1,
                  chains=4,
                  iter=2000,
                  cores=4)

sentcompstane2<-stan_glmer(target~Distance*Predictability+
                    (1+Distance*Predictability|subject)+
                    (1+Distance*Predictability|item.ID),
                  family=binomial(),
                  prior_intercept=student_t(df=2),
                  prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  exp2,
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4)

save(sentcompstane1,file="sentcompstane1.Rda")
save(sentcompstane2,file="sentcompstane2.Rda")
}

load("sentcompstane1.Rda")
load("sentcompstane2.Rda")

## function for extracting results:
summary_stan_model<-function(mod,nfixefs=3){
  samples_m1 <- as.data.frame(mod)
  nfixefs<-nfixefs+1 ## including intercept
  mns<-rep(NA,nfixefs)
  ci95<-matrix(rep(NA,nfixefs*2),ncol=2)
  for(i in 1:nfixefs){
  condnames<-colnames(samples_m1)[1:nfixefs]
  condnames[1]<-"Intercept"
  mns[i]<-round(mean(samples_m1[,i]),digits=4) 
  ci95[i,]<-round(quantile(probs=c(0.025,0.975),samples_m1[,i]),digits=4)
  }
  ## prob less than 0
  prob_less<-rep(NA,nfixefs)
  for(i in 1:nfixefs){
  prob_less[i]<-round(mean(samples_m1[,i]<0),digits=4)
  }
  res<-as.data.frame(cbind(condnames,mns,ci95,prob_less))
  colnames(res)<-c("comparison","mean","lower","upper","P(b<0)")
  return(res)
}

sce1_xtab<-summary_stan_model(sentcompstane1)
sce2_xtab<-summary_stan_model(sentcompstane2)
@

The materials were exactly the same as the ones used in the experiments presented below. For the experiment 1 items, the average prediction accuracy for the exact verb in the strong predictability conditions was 
\Sexpr{sentcomp_mn_exp1[1]}\%
for the short condition and 
\Sexpr{sentcomp_mn_exp1[2]}\%
for the long condition; for the experiment 2 items, it was 
\Sexpr{sentcomp_mn_exp2[1]}\%
and 
\Sexpr{sentcomp_mn_exp2[2]}\%
for the short and long conditions respectively. By contrast, the average prediction accuracy for the exact verb in the weak predictability conditions in experiment 1 was 
\Sexpr{sentcomp_mn_exp1[3]}\%
and 
\Sexpr{sentcomp_mn_exp1[4]}\%
for the short and long conditions; and in experiment 2, it was 
\Sexpr{sentcomp_mn_exp2[3]}\%
and 
\Sexpr{sentcomp_mn_exp2[4]}\%
for the short and long conditions. 
As shown in Tables~\ref{tab:sentcompe1} and \ref{tab:sentcompe2}, an analysis using Bayesian generalized linear mixed models with a binomial link function shows a main effect of predictability in both the first experiment
and the second experiment. 

In the Bayesian models, we used weakly informative priors
for the fixed effects (a Student t-distribution with 2 degrees of freedom), and for the random effects (a so-called LKJ prior on the correlation matrix of the random effects' variance-covariance matrix). For an introduction specifically for psycholinguistics, see \citealp{SorensenVasishthTutorial,NicenboimVasishthStatMeth}.
One way to interpret whether there is an effect of a particular factor in Bayesian (G)LMMs is to check that the 95\% uncertainty interval does not contain zero.


\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{tab:sentcompe1} Table \arabic{table}. }{Model results from the Bayesian linear mixed model for the sentence completion study (Expt 1). Shown are the mean and 95\% uncertainty intervals, and the probability of the parameter being less than 0.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept  & \Sexpr{sce1_xtab[1,2]} & \Sexpr{sce1_xtab[1,3]} & \Sexpr{sce1_xtab[1,4]} & \Sexpr{sce1_xtab[1,5]}\\
Distance  & \Sexpr{sce1_xtab[2,2]} & \Sexpr{sce1_xtab[2,3]} & \Sexpr{sce1_xtab[2,4]} & \Sexpr{sce1_xtab[2,5]}\\
Predictability  & \Sexpr{sce1_xtab[3,2]} & \Sexpr{sce1_xtab[3,3]} & \Sexpr{sce1_xtab[3,4]} & \Sexpr{sce1_xtab[3,5]}\\
Distance x Predictability  & \Sexpr{sce1_xtab[4,2]} & \Sexpr{sce1_xtab[4,3]} & \Sexpr{sce1_xtab[4,4]} & \Sexpr{sce1_xtab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}


\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{tab:sentcompe2} Table \arabic{table}. }{Model results from the Bayesian linear mixed model for the sentence completion study (Expt 2). Shown are the mean and 95\% uncertainty intervals, and the probability of the parameter being less than 0.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept  & \Sexpr{sce2_xtab[1,2]} & \Sexpr{sce2_xtab[1,3]} & \Sexpr{sce2_xtab[1,4]} & \Sexpr{sce2_xtab[1,5]}\\
Distance  & \Sexpr{sce2_xtab[2,2]} & \Sexpr{sce2_xtab[2,3]} & \Sexpr{sce2_xtab[2,4]} & \Sexpr{sce2_xtab[2,5]}\\
Predictability  & \Sexpr{sce2_xtab[3,2]} & \Sexpr{sce2_xtab[3,3]} & \Sexpr{sce2_xtab[3,4]} & \Sexpr{sce2_xtab[3,5]}\\
Distance x Predictability  & \Sexpr{sce2_xtab[4,2]} & \Sexpr{sce2_xtab[4,3]} & \Sexpr{sce2_xtab[4,4]} & \Sexpr{sce2_xtab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}


As is clear from the mean percentages for each condition, the light verbs used in the complex predicate conditions were relatively predictable, and the heavy verbs used in the simple predicate conditions were relatively unpredictable.  It is also clear from this study that, in our materials, increasing the amount of intervening material does not render the upcoming verb more predictable. The additional information provided by the intervening material for predicting the upcoming verb has been suggested by  \citet{konieczny2000locality} as one possible explanation for shorter reading times at the verb in long- vs short-distance conditions. Although this proposal is likely to be correct for some constructions (see discussion in \citealt{grodner2005consequences}),  in our materials, the sentence completion data do not provide any evidence that the intervening words we used in our design sharpen the expectation for the verb.\footnote{In fact, in our sentence completion data, as discussed in the General Discussion, entropy increases with distance.}

\subsection{Acceptability rating of separable vs inseparable CPs}

Because the noun-verb sequences must be  separable for our design to work, we also carried out an acceptability rating pre-test to make sure that the separability of the complex predicates used in our study is acceptable to native speakers. We tested for the acceptability of different types of noun-verb dependencies by interposing a short prepositional phrase between them.  
Taking Karimi-Doostan's classification of complex predicates into account,
36 items from each of the three categories were selected and randomized to test 50 native speakers of Persian (these participants did not take part in any other experiments reported here). They were asked to rate the sentences from 1 (unacceptable) to 7 (completely acceptable). 
Every participant saw all items.
The average acceptability ratings for predicative verbal nouns, predicative nouns and non-predicative nouns were 3.23 (first quartile 1, third quartile 5), 6.08 (first quartile 6, third quartile 7), and 3.12 (first quartile 1, third quartile 5) respectively. That is, items with predicative nouns were the most acceptable. 
We used all the 36 items of the predicative noun condition in our experiments 1, 2, and 32 items in experiments 3, 4 (see the Methods section of experiment 3 for an explanation). 

\section{Experiment 1}


\subsection{Method}
\subsubsection{Participants}

Forty-two participants aged between 17-40 years old (mean 24 years) participated in this experiment in Tehran, Iran. All participants were native speakers of Persian and were unaware of the purpose of the study. This study was carried out in accordance with the Helsinki Declaration, and letters of consent were obtained from all the participants.


\subsubsection{Materials}

We created 36 experimental sentences with a $2 \times 2$ factorial design, manipulating predictability strength and distance between the object noun and verb. The short intervener was a prepositional phrase and the long intervener was a relative clause added before the prepositional phrase. In order to mask the experiment, we included 100 filler sentences with varying syntactic structures (see supplementary materials). Here is an example of the target sentences:

\begin{exe}
\ex
\begin{xlist}
\item Strong predictability, short distance (PP)
\gll Ali a:rezouyee bara:ye man kard va\dots\\
     Ali wish-INDEF for 1.S do-PST and\dots \\
\glt    `Ali made a wish for me and\dots' \\

\item Strong predictability, long distance (RC+PP)
\gll Ali a:rezouyee ke besya:r doost-da:sht-am bara:ye man kard va\dots \\
      Ali wish-INDEF that {a lot} like-1.S-PST for 1.S do-PST and\dots \\
\glt `Ali made a wish that I liked a lot for me and\dots' \\

\item Weak predictability, short distance (PP)
\gll Ali shokola:ti bara:ye man xarid va \dots \\
      Ali chocolate-INDEF for 1.S buy-PST and\dots\\
\glt `Ali bought a chocolate for me and \dots' \\

\item  Weak predictability, long distance (RC+PP)
\gll Ali shokola:ti ke besya:r doost-da:sht-am bara:ye man xarid va\dots \\     
       Ali chocolate-INDEF that {a lot} like-1.S-PST for 1.S buy-PST and\dots\\
\glt  `Ali bought a chocolate that I liked a lot for me and\dots.' 
\end{xlist}
\end{exe}

The critical region is the verb  (\textit{kard} and \textit{xarid}).

Each sentence (including fillers) was followed by a yes/no comprehension question which targeted different thematic roles in the sentence. Half the questions had a yes answer and half had a no answer. The questions used for the target sentences are provided in the supplementary material.

\subsubsection{Procedure}

Participants were tested individually using a PC. They were explained the task before they performed the self-paced reading (SPR) experiment. The participants were instructed to read for comprehension in a normal manner and had a practice session of five sentences. All the sentences were displayed on a single line and were presented in 22 pt Persian Arial font using Linger software (http://tedlab.mit.edue/~dr/Linger/). In order to read each word of a sentence successively in a moving window display, participants had to press the space bar; then the word seen previously was masked and the next word was shown. After each sentence, they were asked to answer a comprehension question to ensure that the participants paid attention to the complete sentence.

\subsubsection{Data analysis}

The data analysis was conducted in the R programming environment \citep{R}, using Bayesian hierarchical (so-called linear mixed) models using Stan \citep{rstanarm2016,stan-manual:2014}.
Sum contrasts were used to code main effects and interactions.
In addition, a nested contrast was defined for a secondary analysis in order to look at the effect of distance in complex predicates vs.\ the control conditions separately; these were also coded as sum contrasts. We fit full variance-covariance matrices for participants and items (the so-called maximal model, \citealp{barr2013random,BatesEtAlParsimonious}).
All data and code are available from 
http://www.ling.uni-potsdam.de/$\sim$vasishth/code/SafaviEtAl2016DataCode.zip.

<<exp1analysis,echo=FALSE>>=
data1<-read.table("data_spr/Persiane1.txt", header=T)

data1$pos<-factor(data1$pos)
data1$resp<-factor(data1$resp)
data1$cond<-factor(data1$cond)

#questions (response accuracy)
data1.q <- subset(data1, pos=="?")
data1.q$resp<- as.numeric(
  as.character(data1.q$resp))
meansq<-round(100*with(data1.q,
                       tapply(resp,cond,mean)),
              digits=1)

data1.q$dist<-ifelse(data1.q$cond%in%c("a","c"),-1,1)
## pred 1, -1
data1.q$pred<-ifelse(data1.q$cond%in%c("a","b"),1,-1)

m1glmer<-glmer(resp~dist*pred+(1|subj)+(1|item),family=binomial(),data1.q)
#summary(m1glmer)

if(0){
stanglmerm1<-stan_glmer(resp~dist*pred+(1+dist+pred|subj)+
               (1+dist+pred|item),
               family=binomial(),
                     prior_intercept=student_t(df=2),
                     prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=data1.q)

save(stanglmerm1,file="stanglmerm1.Rda")
}


load("stanglmerm1.Rda")
stanglmerm1_tab<-summary_stan_model(stanglmerm1)

## Reading times analysis:
## short -1, long 1
data1$dist<-ifelse(data1$cond%in%c("a","c"),-1,1)
## pred 1, -1
data1$pred<-ifelse(data1$cond%in%c("a","b"),1,-1)

#nested
data1$pred.dist <- ifelse(data1$cond=="a",-1,
                          ifelse(data1$cond=="b",1,0))
data1$nopred.dist <- ifelse(data1$cond=="c",-1,
                         ifelse(data1$cond=="d",1,0))

## 4 extreme data points removed: 0.02%

m1<-lmer(log(rt)~dist*pred+(1+dist+dist:pred||subj)+(1+pred||item),subset(data1,roi=="crit" & rt<3000))
#summary(m1)

if(0){
stanm1<-stan_lmer(log(rt)~dist*pred+(1+dist*pred|subj)+
               (1+dist*pred|item),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.99,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data1,roi=="crit" & rt<3000))

save(stanm1,file="stanm1.Rda")

stanm1nested<-stan_lmer(log(rt)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist|subj)+
               (1+pred+pred.dist+nopred.dist|item),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.99,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data1,roi=="crit" & rt<3000))

save(stanm1nested,file="stanm1nested.Rda")
}

load("stanm1.Rda")
load("stanm1nested.Rda")

stanm1_tab<-summary_stan_model(stanm1)
stanm1nested_tab<-summary_stan_model(stanm1nested)

## create e1crit for comparison with exp2:
e1crit<-subset(data1,roi="crit")
@


\subsection{Predictions}

Based on the \citet{husain2014strong} results, 
in experiment 1, 
we expected that increasing noun-verb distance would lead to faster reading time at the verb in the strong predictable conditions, but slower reading time in the weak predictable conditions. Thus, we expected to obtain a cross-over interaction. 

The memory based accounts \citep{just1992ctc,gibson2000dependency,lewis2005activation} predict that increasing distance should lead to a slowdown at the verb; these accounts make no predictions about  the strength of predictability.

There are two alternative predictions possible for the expectation account, depending on how one operationalizes expectation. First, if sentence completion probabilities are a reasonable proxy for conditional probabilities---and the previous research reported above \citep{husain2014strong,levy2013expectation,Jaegeretal2015} suggests that they may be---then we predict (a) no difference in reading time at the verb as a function of distance, and (b) faster reading time at the verb in the strong predictable conditions than the weak predictable conditions. Prediction (a) arises because, in the sentence completion data, we see no effect of distance on the predictability of the upcoming verb, in either the strong or weak predictability conditions; prediction (b) arises due to the difference in predictability of the exact verb that we see in the strong versus weak predictability conditions (see the results of the sentence completion studies).

An alternative possible prediction of the expectation account is that increasing distance should facilitate processing at the verb. Surprisal predicts facilitation with increasing distance whenever distance causes the number of possible parses to decrease; this decrease in the number of possible parses leads to the probability mass being reassigned among the remaining parses. 
In our materials,  when a participant reads the noun in the noun-verb complex predicate, they are expecting the light verb with high probability (nearly 1). However, in the long distance condition, the next word begins a relative clause; this leads to an expectation that the light verb will appear \textit{after} the relative clause verb. But what appears after the relative clause verb is a PP that modifies the upcoming light verb.  For a facilitation to be predicted in this long-distance condition by the surprisal metric, it would have to be the case that the conditional probability of the light verb following the RC and PP would be higher than the conditional probability of the light verb in the short-distance (PP) condition. 
In order to get a sense of how the conditional probabilities change in the noun-light verb conditions as a function of distance, we extracted all light verb sentences from a Persian corpus \citep{Mojganphd} and then counted, for different numbers of modifying phrases, the proportion of cases that a verb followed the intervening phrase.  For example, in a Persian sentence such as \textit{John in the morning went}, there is one intervening phrase, the PP.
As shown in Table~\ref{tab:corpuscountlightverb}, we find that the conditional probability of the verb appearing next is always high, but goes to 1 with increasing distance. This suggests that in general, increasing distance tends to sharpen the expectation for an upcoming verb.  
We also did this calculation using the number of intervening words as a metric, rather than the number of intervening phrases. The result, shown in Table~\ref{tab:corpuscountlightverb2}, is substantially the same as in Table~\ref{tab:corpuscountlightverb}.
Of course, these corpus counts don't give us any direct information about the predictions regarding our particular experiment design.

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{tab:corpuscountlightverb} Table \arabic{table}. }{ The conditional probability of a light verb appearing given the complex predicate noun and $n$ intervening phrases between the noun and the light verb.}
\processtable{}
{\begin{tabular}{cr}\toprule
$n$ intervening phrases & probability of verb\\
\midrule
0                          &  $3826/4003 = 0.95$\\
1                          & $131/133 = 0.98$\\
2                          &  $28/31 = 0.90$\\
3                    & $5/5 = 1$\\
4                    & $2/2 = 1$\\
6                    &  $1/1 = 1$\\
\botrule
\end{tabular}}{}
\end{table}

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{tab:corpuscountlightverb2} Table \arabic{table}. }{ The conditional probability of a light verb appearing given the complex predicate noun and $n$ intervening words between the noun and the light verb.}
\processtable{}
{\begin{tabular}{cr}\toprule
$n$ intervening words & probability of verb\\
0 &   3826/4003 = 0.96\\
1 &   104/104 =         1\\
2  &  36/39      =    0.92\\
3  &  4/5           =   0.8\\
4  &  9/10          =    0.9\\
5  &  3/3            =  1\\
6  &  3/3            =  1\\
7  &  1/1            =  1\\
8  &  2/2             = 1\\
10 &   2/2           =   1\\
12  &  1/1            =  1\\
13   & 1/1            =  1\\
14   & 1/1            =  1\\
\botrule
\end{tabular}}{}
\end{table}

Regarding the strong vs.\ weak predictability conditions, 
note that the expectation account of Hale and Levy does not predict that processing should be facilitated when the exact identity of the upcoming verb is predicted (strong predictability condition), compared to the case when just some verb is predicted (weak predictability condition). This is because the surprisal metric is usually calculated using the conditional probability of the part-of-speech (verb) given preceding context, and this will be the same in both the strong and weak predictability conditions. However, it is possible to subsume the difference between strong and weak predictability under the surprisal account by reframing the conditional probabilities in terms of the exact identity of the verb. In this case, the expectation account would predict faster reading times in the strong predictability conditions compared to the weak predictability conditions, regardless of distance. 

To summarize, regarding the distance manipulation, the expectation account predicts either no effect or a facilitation at the verb as a function of distance; and regarding the predictability manipulation, the expectation account (appropriately formulated to include the conditional probability of the exact lexical item predicted) would predict a main effect of predictability. 

\subsection{Results}

\subsubsection{Comprehension accuracy}
Participants answered correctly on average 
\Sexpr{round(mean(meansq),digits=0)} percent of all comprehension questions (excluding fillers). Accuracy was 
\Sexpr{round(meansq[1],digits=0)}, \Sexpr{round(meansq[2],digits=0)}, \Sexpr{round(meansq[3],digits=0)}, and \Sexpr{round(meansq[4],digits=0)}
percent respectively for the four conditions in (1). As shown in Table~\ref{Tab:01q}, 
a Bayesian generalized linear mixed model of the binary responses showed no evidence for an effect of distance or predictability, or an interaction 
between predictability and distance. 

\subsubsection{Reading time}

Reading times (RTs) were analyzed at the verb.  As shown in Table~\ref{Tab:01} and Figure~\ref{fig:SPR1-interaction}, there was a main effect of distance, such that increasing distance led to longer reading times. There was also a main effect of predictability: the complex predicate conditions were read faster overall. A weak interaction was also seen: stronger locality effects were seen in the control conditions than in the complex predicate conditions. 
A nested analysis shows that the distance effect was driven by the control (weak predictability) condition.
The estimates for the strong predictability condition were 
coef.=\Sexpr{stanm1nested_tab[3,2]}, [\Sexpr{stanm1nested_tab[3,3]}, \Sexpr{stanm1nested_tab[3,4]}], $P(b<0)$=\Sexpr{stanm1nested_tab[3,5]});
and the estimates for 
weak predictability were 
coef.=\Sexpr{stanm1nested_tab[4,2]}, 95\% uncertainty intervals [\Sexpr{stanm1nested_tab[4,3]}, \Sexpr{stanm1nested_tab[4,4]}], $P(b<0)$=\Sexpr{stanm1nested_tab[4,5]}.

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{Tab:01q} Table \arabic{table}. }{Means, 95\% uncertainty intervals, and $P(b<0)$, the probability of the estimate being less than 0, in the question-response accuracy analysis for experiment 1.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept  & \Sexpr{stanglmerm1_tab[1,2]} & \Sexpr{stanglmerm1_tab[1,3]} & \Sexpr{stanglmerm1_tab[1,4]} & \Sexpr{stanglmerm1_tab[1,5]}\\
Distance  & \Sexpr{stanglmerm1_tab[2,2]} & \Sexpr{stanglmerm1_tab[2,3]} & \Sexpr{stanglmerm1_tab[2,4]} & \Sexpr{stanglmerm1_tab[2,5]}\\
Predictability  & \Sexpr{stanglmerm1_tab[3,2]} & \Sexpr{stanglmerm1_tab[3,3]} & \Sexpr{stanglmerm1_tab[3,4]} & \Sexpr{stanglmerm1_tab[3,5]}\\
Distance x Predictability  & \Sexpr{stanglmerm1_tab[4,2]} & \Sexpr{stanglmerm1_tab[4,3]} & \Sexpr{stanglmerm1_tab[4,4]} & \Sexpr{stanglmerm1_tab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{Tab:01} Table \arabic{table}. }{Means, 95\% uncertainty intervals, and $P(b<0)$, the probability of the estimate being less than 0, in the reading time analysis for experiment 1.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept  & \Sexpr{stanm1_tab[1,2]} & \Sexpr{stanm1_tab[1,3]} & \Sexpr{stanm1_tab[1,4]} & \Sexpr{stanm1_tab[1,5]}\\
Distance  & \Sexpr{stanm1_tab[2,2]} & \Sexpr{stanm1_tab[2,3]} & \Sexpr{stanm1_tab[2,4]} & \Sexpr{stanm1_tab[2,5]}\\
Predictability  & \Sexpr{stanm1_tab[3,2]} & \Sexpr{stanm1_tab[3,3]} & \Sexpr{stanm1_tab[3,4]} & \Sexpr{stanm1_tab[3,5]}\\
Distance x Predictability  & \Sexpr{stanm1_tab[4,2]} & \Sexpr{stanm1_tab[4,3]} & \Sexpr{stanm1_tab[4,4]} & \Sexpr{stanm1_tab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}



\begin{figure}[!htbp]
<<plotexpt1,echo=FALSE>>=
data1 <- subset(data1, pos!='?')

a.regions<-c("predep","dep","precrit","crit","postcrit","post")
b.regions<-c("predep","dep","precrit","crit","postcrit","post")
c.regions<-c("predep","dep","precrit","crit","postcrit","post")
d.regions<-c("predep","dep","precrit","crit","postcrit","post")

regions<-c(a.regions,b.regions,c.regions,d.regions)
region.id<-rep(1:6,4)
cond.id<-rep(letters[1:4],each=6)

region.df<-data.frame(cond=factor(cond.id),
                      region.id=region.id,
                      roi=factor(regions))

#merge multiple regions with same id
data1.uniq.reg<-ddply(data1, 
                     .(subj, item, cond, roi),
                     summarize, 
                     rt = sum(rt))

data1.merged<-merge(data1.uniq.reg,region.df, 
                   by.x=c("cond","roi"))
data1.merged$cond<-droplevels(data1.merged$cond)

Expectation<-factor(ifelse(data1.merged$cond%in%c("a","b"),"strong-exp","weak-exp"),
             levels=c("strong-exp","weak-exp"))
dist<-factor(ifelse(data1.merged$cond%in%c("a","c"),"short","long"),
                levels=c("short","long"))

data1.merged$Expectation<-Expectation
data1.merged$dist<-dist

data.rs <- melt(data1.merged, 
                id=c("Expectation","dist","cond","roi",
                     "region.id","subj"), 
                measure=c("rt"),
                na.rm=TRUE)


#get mean rt and no. of data points for each region, for each subject/condition
data.id  <- data.frame(cast(data.rs, 
                            subj+Expectation+dist+cond+roi+region.id ~ ., 
                            function(x) c(rt=mean(x), N=length(x) ) ))

#mean of mean rt of each subject
GM <- mean(tapply(data.id$rt, data.id$subj, mean))

#deviation from the grandmean after considering intra-subject variability
#removing between subject variance
data.id <- ddply(data.id, .(subj), 
                 transform, rt.w = rt - mean(rt) + GM)

temp<-melt(data.id, id.var=c("subj","Expectation","dist","cond","roi","region.id"), 
           measure.var="rt.w")

M.id.w <- cast(temp, Expectation+dist+cond+roi+region.id  ~ ., 
               function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )
M.id.w.pred <- cast(temp, Expectation+roi+region.id  ~ ., 
                    function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

k<-1

#head(M.id.w)
M.id.w.orig<-M.id.w
## make names consistent with paper:
colnames(M.id.w)[1]<-"Predictability"
M.id.w$Predictability<-factor(ifelse(M.id.w$Predictability=="strong-exp","strong","weak"))

p1a<-ggplot(subset(M.id.w,roi=="crit"), 
             aes(x=dist, y=M,group=Predictability)) + 
               geom_point(shape=21,fill="white",size=k*3) +
               geom_line(aes(linetype=Predictability),size=k) +
               geom_errorbar(aes(ymin=M-2*SE, ymax=M+2*SE),
                             width=.1,size=k)+
                               xlab("Distance")+
                               ylab("reading time [RT in ms]")+                                               labs(title="Critical region [Verb]") + theme_bw() + labs(legend.position=c(.87, .6))

p1a+theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+
  theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))
@
\caption{Reading times at the critical verb in experiment 1.}\label{fig:SPR1-interaction}
\end{figure}


\subsection{Discussion}

Experiment 1 found a main effect of predictability such that the strong predictability conditions were read faster than the weak predictability conditions, and a main effect of distance, such that the short conditions were read faster than the long conditions. A nested contrast showed that this effect of distance was driven by the weak predictability conditions, i.e., reading time at the verb in condition c was faster than the reading time in condition d. A weak interaction suggests that the locality effect may be somewhat stronger in the weak predictability condition. The suggestion of an interaction seems to provide only weak support, if any, for the idea that strong predictability can at least attenuate locality effects \citep{husain2014strong}.
The overall effect of distance is consistent with memory-based accounts, which correctly predict a slowdown at the verb in the long conditions, i.e., a main effect of distance. However, as the nested comparison shows, the main effect of distance is driven only by the weak predictability (non-complex predicate) conditions. Memory-based theories would be unable to explain this because they predict a slowdown in long conditions irrespective of predictability strength. However, note
that the absence of an interaction makes this absence of
a distance effect in the strong predictability conditions difficult to interpret.
The expectation account's prediction regarding distance, that increasing the argument-verb distance would either have no effect or result in a facilitation, was clearly not validated; however, the main effect of predictability is consistent with a version of the expectation account that uses the conditional probability of the exact lexical item (verb) appearing given the preceding context.

Our original motivation for this study was to attempt a replication of the \cite{husain2014strong} findings.  
The results are not entirely inconsistent with those of \cite{husain2014strong}, but they are also not a strong validation of the expectation-memory cost tradeoff posited in that paper. As in the Husain et al.\ study, we see a main effect of predictability driven by the complex predicate condition. This effect could be explained in terms of reduced retrieval cost at the verb due to its high expectation. An  obvious confounding factor here is that the verbs in the strong vs weak predictability conditions are not identical; this prevents us from ruling out the possibility that low-level differences in the verbs might be responsible for the facilitation due to prediction strength.

We turn next to experiment 2, in which we manipulate the type of intervener. Here, in the long distance condition, instead of  a relative clause and prepositional phrase (PP) intervener,  a long PP intervenes. The motivation was to increase distance without having different types of interveners in the short vs long conditions, as this might be a fairer comparison.

\section{Experiment 2}

<<exp2analysis,echo=FALSE>>=
data2<-read.table("data_spr/persiane2.txt", header=T)

data2$pos<-factor(data2$pos)
data2$resp<-factor(data2$resp)
data2$cond<-factor(data2$cond)

## Question response accuracy
data2.q <- subset(data2, pos=="?")
data2.q$resp<- as.numeric(as.character(data2.q$resp))
meansq<-round(100*with(data2.q,tapply(resp,cond,mean)),digits=1)

###accuracy analysis
data2.q$dist<-ifelse(data2.q$cond%in%c("a","c"),-1,1)
## pred 1, -1
data2.q$pred<-ifelse(data2.q$cond%in%c("a","b"),1,-1)

#m2glmer<-glmer(resp~dist*pred+(1|subj)+(1|item),family=binomial(),data2.q)
#summary(m2glmer)
## cf:
#m1glmer<-glmer(resp~dist*pred+(1+dist*pred||subj)+(1|item),family=binomial(),data1.q)
#summary(m1glmer)

if(0){
stanglmerm2<-stan_glmer(resp~dist*pred+(1+dist+pred|subj)+
               (1+dist+pred|item),
               family=binomial(),
                     prior_intercept=student_t(df=2),
                     prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=data2.q)

save(stanglmerm2,file="stanglmerm2.Rda")
}

load("stanglmerm2.Rda")
stanglmerm2_tab<-summary_stan_model(stanglmerm2)

## short -1, long 1
data2$dist<-ifelse(data2$cond%in%c("a","c"),-1,1)
## pred 1, -1
data2$pred<-ifelse(data2$cond%in%c("a","b"),1,-1)

#nested
data2$pred.dist <- ifelse(data2$cond=="a",-1,
                          ifelse(data2$cond=="b",1,0))
data2$nopred.dist <- ifelse(data2$cond=="c",-1,
                            ifelse(data2$cond=="d",1,0))

## 6 extreme data points removed: 0.4%
#m2<-lmer(log(rt)~dist*pred+(1+dist+dist:pred||subj)+(1+dist+dist:pred||item),subset(data2,roi=="crit"))

#m2<-lmer(log(rt)~dist*pred+(1+dist+dist:pred||subj)+(1+dist+dist:pred||item),subset(data2,roi=="crit" & rt<3000))
#summary(m2)
#qqPlot(residuals(m2))

#Nothing much changes if we remove items 5, 9, 26, 32, which had low values:

#data2subset<-subset(data2,item!=5 & item!=9 & item!=26 & item!=32)
#length(sort(unique(data2subset$item)))

#m2a<-lmer(log(rt)~dist*pred+(1+dist+dist:pred||subj)+(1+dist+dist:pred||item),subset(data2subset,roi=="crit" & rt<3000))
#summary(m2)

#m2.nested<-lmer(log(rt)~pred+pred.dist+nopred.dist+(1+pred.dist+nopred.dist||subj)+(1+pred.dist+nopred.dist||item),subset(data2,roi=="crit" & rt<3000))
#summary(m2.nested) 

## main and nested analysis
if(0){
stanm2<-stan_lmer(log(rt)~dist*pred+(1+dist*pred|subj)+
               (1+dist*pred|item),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.99,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data2,roi=="crit" & rt<3000))

save(stanm2,file="stanm2.Rda")

stanm2nested<-stan_lmer(log(rt)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist|subj)+
               (1+pred+pred.dist+nopred.dist|item),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data2,roi=="crit" & rt<3000))

save(stanm2nested,file="stanm2nested.Rda")
}

load("stanm2.Rda")
load("stanm2nested.Rda")

stanm2_tab<-summary_stan_model(stanm2)
stanm2nested_tab<-summary_stan_model(stanm2nested)

#Extract data from critical region for comparing with expt 1:

e2crit<-subset(data2,roi=="crit")
e1crit$expt<-factor("e1")
e1crit$subj<-paste("e1",e1crit$subj,sep="")
e2crit$expt<-factor("e2")
e2crit$subj<-paste("e2",e2crit$subj,sep="")
e1e2crit<-rbind(e1crit,e2crit)
e1e2crit$expt<-ifelse(e1e2crit$expt=="e1",-1,1)
e1e2crit$subj<-factor(e1e2crit$subj)

## Is there an interaction of expt with distance and pred: yes
me1e2<-lmer(log(rt)~ dist*pred+expt + dist:expt + pred:expt + dist:pred:expt + (1+dist*pred||subj) + (1+dist*pred||item),
            subset(e1e2crit,roi=="crit" & rt<3000))
#round(summary(me1e2)$coefficients,digits=2)

if(0){
stanm2combined<-stan_lmer(log(rt)~dist*pred+expt + dist:expt + pred:expt + dist:pred:expt+(1+dist*pred|subj)+
               (1+dist*pred|item),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.99,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(e1e2crit,roi=="crit" & rt<3000))

#print(stanm2combined)

save(stanm2combined,file="stanm2combined.Rda")

}
load("stanm2combined.Rda")

stanm2combined_tab<-summary_stan_model(stanm2combined,nfixefs=7)
@




\subsection{Method}
\subsubsection{Participants}

Forty-three participants, with  the same criteria as in experiment 1,  participated in this experiment in Tehran, Iran. This study was carried out in accordance with Helsinki Declaration, and  consent forms were obtained from all the participants. 

\subsubsection{Materials}

The stimuli and fillers were the same as in experiment 1 except for the long conditions (b and d), where the intervener was a longer prepositional phrase (PP) instead of the combination of a relative clause and a PP as in the previous experiment. 
The PP was lengthened using several different structures, all of which had one or more instance of the ezafe possessive marker \citep{samvelian2007EZ}:

\begin{enumerate}
\item
N-ez N-ez N/pronoun/proper name
\item
N-ez adj-ez N/pronoun/proper name
\item
N-ez adj-ez N
\item
N-ez N-ez adj
\item
 N adj-ez adj
 \item
 superlative adj N N/pronoun/proper name
 \item  N-ez pronoun
 \end{enumerate}
 
One set of examples using the first type of PP shown above is as follows:

\noautomath

\begin{exe}
\ex
\begin{xlist}
\item Strong predictability, short distance (PP)
\gll Ali a:rezouyee bara:ye man kard va\dots\\
Ali wish-INDEF for 1.S do-PST and\dots\\
\glt 'Ali made a wish for me and\dots'\\

\item Strong predictability, long distance (longer PP)
\gll Ali a:rezouyee bara:ye doost-e xa:har-e man kard va\dots\\
Ali wish-INDEF for friend-EZ sister-EZ 1.S do-PST and\dots\\
\glt 'Ali made a wish for my sister's friend\dots'\\
    
\item Weak predictability, short distance (PP)
\gll Ali shokola:ti bara:ye man xarid va\dots\\
Ali chocolate-INDEF for 1.S buy-PST and\dots\\
\glt 'Ali bought a chocolate for me and\dots'\\
    
\item weak predictability, long distance (longer PP)
\gll Ali shokola:ti bara:ye doost-e xa:har-e man xarid va\dots\\
Ali chocolate-INDEF for friend-EZ sister-EZ 1.S buy-PST and\dots\\
\glt 'Ali bought a chocolate for my sister's friend and\dots'\\
\end{xlist}
\end{exe}

More details about the PPs are provided in the Supplementary materials.

\subsubsection{Procedure and Data Analysis}

The procedure and data analysis methodology was the same as experiment 1.

 \subsection{Predictions}

In experiment 2, the distance manipulation involves lengthening the PP. There are two possible predictions of surprisal. One is that
surprisal may predict no difference at the verb; this would be because the end of the PP raises a strong expectation for a verb, and this strong expectation for a verb would be the same in both the short and long PP conditions. Another alternative possible prediction of surprisal is that lengthening the PP could lead to a facilitation. This prediction could hold if increasing distance, counted in terms of the number of intervening words, generally increases the predictability of the upcoming verb; this is a possibility given the corpus counts in Table~\ref{tab:corpuscountlightverb2}.



\subsection{Results}
\subsubsection{Comprehension Accuracy}

Participants answered \Sexpr{round(mean(meansq))}\% of all comprehension questions correctly on average (excluding fillers). The accuracies by condition were 
\Sexpr{round(meansq[1])}, 
\Sexpr{round(meansq[2])}, 
\Sexpr{round(meansq[3])}, and
\Sexpr{round(meansq[4])}\% 
percent respectively for the four conditions in (2). 
As shown in Table~\ref{Tab:02q}, 
the Bayesian generalized linear mixed models of the responses showed a main effect of distance,
such that accuracies were lower in the long conditions. No effect of predictability strength, and no interaction between predictability strength and distance were found. 


\subsubsection{Reading Time}
As shown in Table~\ref{Tab:02} and Figure~\ref{fig:SPR2-interaction}, 
the results showed a main effect of distance, with long distance conditions being read slower. There was only a weak effect of predictability, with the strong predictability condition being read faster than the weak predictability condition. No interaction was found between predictability and distance. A nested contrast showed that the distance effect is seen in both  strong predictability 
(coef.=\Sexpr{stanm2nested_tab[3,2]}, [\Sexpr{stanm2nested_tab[3,3]}, \Sexpr{stanm2nested_tab[3,4]}], $P(b<0)$=\Sexpr{stanm2nested_tab[3,5]})
and weak predictability 
(coef.=\Sexpr{stanm2nested_tab[4,2]}, [\Sexpr{stanm2nested_tab[4,3]}, \Sexpr{stanm2nested_tab[4,4]}], $P(b<0)$=\Sexpr{stanm2nested_tab[4,5]})
conditions. 

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{Tab:02q} Table \arabic{table}. }{Means, 95\% uncertainty intervals, and $P(b<0)$, the probability of the estimate being less than 0, in the question-response accuracy analysis for experiment 2.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept  & \Sexpr{stanglmerm2_tab[1,2]} & \Sexpr{stanglmerm2_tab[1,3]} & \Sexpr{stanglmerm2_tab[1,4]} & \Sexpr{stanglmerm2_tab[1,5]}\\
Distance  & \Sexpr{stanglmerm2_tab[2,2]} & \Sexpr{stanglmerm2_tab[2,3]} & \Sexpr{stanglmerm2_tab[2,4]} & \Sexpr{stanglmerm2_tab[2,5]}\\
Predictability  & \Sexpr{stanglmerm2_tab[3,2]} & \Sexpr{stanglmerm2_tab[3,3]} & \Sexpr{stanglmerm2_tab[3,4]} & \Sexpr{stanglmerm2_tab[3,5]}\\
Distance x Predictability  & \Sexpr{stanglmerm2_tab[4,2]} & \Sexpr{stanglmerm2_tab[4,3]} & \Sexpr{stanglmerm2_tab[4,4]} & \Sexpr{stanglmerm2_tab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}




\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{Tab:02} Table \arabic{table}. }{Means, 95\% uncertainty intervals, and $P(b<0)$, the probability of the estimate being less than 0, in the reading time analysis for experiment 2.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept  & \Sexpr{stanm2_tab[1,2]} & \Sexpr{stanm2_tab[1,3]} & \Sexpr{stanm2_tab[1,4]} & \Sexpr{stanm2_tab[1,5]}\\
Distance  & \Sexpr{stanm2_tab[2,2]} & \Sexpr{stanm2_tab[2,3]} & \Sexpr{stanm2_tab[2,4]} & \Sexpr{stanm2_tab[2,5]}\\
Predictability  & \Sexpr{stanm2_tab[3,2]} & \Sexpr{stanm2_tab[3,3]} & \Sexpr{stanm2_tab[3,4]} & \Sexpr{stanm2_tab[3,5]}\\
Distance x Predictability  & \Sexpr{stanm2_tab[4,2]} & \Sexpr{stanm2_tab[4,3]} & \Sexpr{stanm2_tab[4,4]} & \Sexpr{stanm2_tab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}


\begin{figure}[!htbp]
<<plotexpt2,echo=FALSE>>=
data2 <- subset(data2, pos!='?')

a.regions<-c("predep","dep","precrit","crit","postcrit","post")
b.regions<-c("predep","dep","precrit","crit","postcrit","post")
c.regions<-c("predep","dep","precrit","crit","postcrit","post")
d.regions<-c("predep","dep","precrit","crit","postcrit","post")

regions<-c(a.regions,b.regions,c.regions,d.regions)
region.id<-rep(1:6,4)
cond.id<-rep(letters[1:4],each=6)

region.df<-data.frame(cond=factor(cond.id),
                      region.id=region.id,
                      roi=factor(regions))

#merge multiple regions with same id
data2.uniq.reg<-ddply(data2, 
                      .(subj, item, cond, roi),
                      summarize, 
                      rt = sum(rt))

data2.merged<-merge(data2.uniq.reg,region.df, 
                    by.x=c("cond","roi"))
data2.merged$cond<-droplevels(data2.merged$cond)

Expectation<-factor(ifelse(data2.merged$cond%in%c("a","b"),"strong-exp","weak-exp"),
             levels=c("strong-exp","weak-exp"))
dist<-factor(ifelse(data2.merged$cond%in%c("a","c"),"short","long"),
             levels=c("short","long"))

data2.merged$Expectation<-Expectation
data2.merged$dist<-dist

data.rs <- melt(data2.merged, 
                id=c("Expectation","dist","cond","roi",
                     "region.id","subj"), 
                measure=c("rt"),
                na.rm=TRUE)


#get mean rt and no. of data points for each region, for each subject/condition
data.id  <- data.frame(cast(data.rs, 
                            subj+Expectation+dist+cond+roi+region.id ~ ., 
                            function(x) c(rt=mean(x), N=length(x) ) ))

#mean of mean rt of each subject
GM <- mean(tapply(data.id$rt, data.id$subj, mean))

#deviation from the grandmean after considering intra-subject variability
#removing between subject variance
data.id <- ddply(data.id, .(subj), 
                 transform, rt.w = rt - mean(rt) + GM)

temp<-melt(data.id, id.var=c("subj","Expectation","dist","cond","roi","region.id"), 
           measure.var="rt.w")

M.id.w <- cast(temp, Expectation+dist+cond+roi+region.id  ~ ., 
               function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )
M.id.w.pred <- cast(temp, Expectation+roi+region.id  ~ ., 
                    function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

k<-1

M.id.w.orig<-M.id.w
## make names consistent with paper:
colnames(M.id.w)[1]<-"Predictability"
M.id.w$Predictability<-factor(ifelse(M.id.w$Predictability=="strong-exp","strong","weak"))

p2a<-ggplot(subset(M.id.w,roi=="crit"), 
             aes(x=dist, y=M,group=Predictability)) + 
   geom_point(shape=21,fill="white",size=k*3) +
   geom_line(aes(linetype=Predictability),size=k) +
   geom_errorbar(aes(ymin=M-2*SE, ymax=M+2*SE),
                 width=.1,size=k)+
   xlab("Distance")+
   ylab("reading time [RT in ms]")+                     
   labs(title="Critical region [Verb]") +
   theme_bw() +
   labs(legend.position=c(.87, .6))

p2a+theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))
@
\caption{Reading times at the critical verb in experiment 2.}\label{fig:SPR2-interaction}
\end{figure}

<<byregionplotexcluded,echo=FALSE,eval=FALSE>>=
M.id.w <- M.id.w.orig

#############

M.id.w.cp<-subset(M.id.w,Expectation=="strong-exp")
M.id.w.cp$Expectation<-droplevels(M.id.w.cp$Expectation)

M.id.w.sp<-subset(M.id.w,Expectation=="weak-exp")
M.id.w.sp$Expectation<-droplevels(M.id.w.sp$Expectation)

## function for generic by-region plots (effect of distance within CP vs. SP):
byregion.plot<-function(data,
                        mytitle,k=1,
                        x.lab="position",
                        y.lab="reading time [Raw rt]"){
  ggplot(data,aes(x=region.id,y=M,
                  group=dist)) + 
    geom_point(shape=21,size=k*2) +
    geom_line(aes(linetype=dist),size=k) +
    geom_errorbar(aes(ymin=M-2*SE, 
                      ymax=M+2*SE),
                  width=.1,size=k)+
    #xlab(x.lab)+
    scale_x_continuous(x.lab, breaks = 1:6, labels = c("predep","dep","precrit","crit","postcrit","post"))+
    #ylab(y.lab)+             
    #opts(title=mytitle) +
    #theme_bw()+
    #theme(axis.text.x = element_text(size=7))
    labs(x ='\nPosition', y = 'Reading time [msec]\n')+
    theme_bw()+
    ggtitle(mytitle)}

(plot.cp<-byregion.plot(M.id.w.cp,
                        mytitle="Effect of distance [strong expectation]",k=1,
                        x.lab="position",y.lab="reading time [Raw rt]")
)


(plot.sp<-byregion.plot(M.id.w.sp,
                        mytitle="Effect of distance [weak expectation]",k=1,
                        x.lab="position",y.lab="reading time [log ms]")
)


M.id.w.short<-subset(M.id.w,dist=="short")
M.id.w.short$dist <- droplevels(M.id.w.short$dist)

M.id.w.long<-subset(M.id.w,dist=="long")
M.id.w.long$dist <- droplevels(M.id.w.long$dist)

byregion.plot<-function(data,
                        mytitle,k=1,
                        x.lab="position",
                        y.lab="reading time [msec]"){
  ggplot(data,aes(x=region.id,y=M,
                  group=Expectation)) + 
    geom_point(shape=21,size=k*2) +
    geom_line(aes(linetype=Expectation),size=k) +
    geom_errorbar(aes(ymin=M-2*SE, 
                      ymax=M+2*SE),
                  width=.1,size=k)+
    #xlab(x.lab)+
    scale_x_continuous(x.lab, breaks = 1:6, labels = c("predep","dep","precrit","crit","postcrit","post"))+
    #ylab(y.lab)+             
    #opts(title=mytitle) +
    #theme_bw()+
    #theme(axis.text.x = element_text(size=7))
    labs(x ='\nPosition', y = 'Reading time [msec]\n')+
    theme_bw()+
    ggtitle(mytitle)}


(byregion.plot(M.id.w.long,
               mytitle="Effect of expectation (long conditions)",k=1,
               x.lab="position",y.lab="reading time [msec]")
)

(byregion.plot(M.id.w.short,
               mytitle="Effect of expectation (short conditions)",k=1,
               x.lab="position",y.lab="reading time [msec]")
)
@

\subsection{Discussion}

In this experiment, we replicated the locality effects found in experiment 1, but we no longer see a weakening of the locality effect that was seen in experiment 1 (a marginal interaction was found in experiment 1). Nested contrasts showed that locality effects are equally strong in both the strong and weak predictability conditions. In experiment 2, we also see an effect of predictability, with the strong predictable verb being read faster.  Thus, regarding the distance manipulation, the working-memory account's prediction is validated, and the expectation-based account's prediction is not supported. The main effect of predictability does furnish evidence consistent with the expectation-based account.

A secondary analysis was conducted to compare the strength of the locality effect in the two experiments, and to determine whether an interaction between distance, predictability and experiment was present. The between-participant factor experiment was coded using sum coding: experiment 1 was coded -1, and experiment 2 was coded +1 (further details are available in the supplementary materials). The results are shown in Table~\ref{Tab:e1e2comp}. There isn't any convincing evidence for an interaction between distance and experiment; there is only weak evidence for a larger effect of distance in experiment 2. We cannot therefore argue for a qualitative difference in the distance effects found in experiments 1 vs.\ 2.

\begin{table}[!htbp]
\centering
\textbf{\refstepcounter{table}\label{Tab:e1e2comp} Table \arabic{table}. }{Comparison of experiments 1 and 2.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
& mean & lower & upper & P(b<0)\\
  \hline
Intercept & 
\Sexpr{stanm2combined_tab[1,2]} &
\Sexpr{stanm2combined_tab[1,3]} &
\Sexpr{stanm2combined_tab[1,4]} &
\Sexpr{stanm2combined_tab[1,5]} \\
Distance & 
\Sexpr{stanm2combined_tab[2,2]} &
\Sexpr{stanm2combined_tab[2,3]} &
\Sexpr{stanm2combined_tab[2,4]} &
\Sexpr{stanm2combined_tab[2,5]} \\
Predictability & 
\Sexpr{stanm2combined_tab[3,2]} &
\Sexpr{stanm2combined_tab[3,3]} &
\Sexpr{stanm2combined_tab[3,4]} &
\Sexpr{stanm2combined_tab[3,5]} \\
Expt & 
\Sexpr{stanm2combined_tab[4,2]} &
\Sexpr{stanm2combined_tab[4,3]} &
\Sexpr{stanm2combined_tab[4,4]} &
\Sexpr{stanm2combined_tab[4,5]} \\
Distance$\times$Predictability  & 
\Sexpr{stanm2combined_tab[5,2]} &
\Sexpr{stanm2combined_tab[5,3]} &
\Sexpr{stanm2combined_tab[5,4]} &
\Sexpr{stanm2combined_tab[5,5]} \\
Distance$\times$ Expt & 
\Sexpr{stanm2combined_tab[6,2]} &
\Sexpr{stanm2combined_tab[6,3]} &
\Sexpr{stanm2combined_tab[6,4]} &
\Sexpr{stanm2combined_tab[6,5]} \\
Predictability$\times$ Expt & 
\Sexpr{stanm2combined_tab[7,2]} &
\Sexpr{stanm2combined_tab[7,3]} &
\Sexpr{stanm2combined_tab[7,4]} &
\Sexpr{stanm2combined_tab[7,5]} \\
Pred$\times$ Dist $\times$ Expt & 
\Sexpr{stanm2combined_tab[8,2]} &
\Sexpr{stanm2combined_tab[8,3]} &
\Sexpr{stanm2combined_tab[8,4]} &
\Sexpr{stanm2combined_tab[8,5]} \\
\botrule
\end{tabular}}{}
\end{table}

In experiment 2, the intervener was a long, uninterrupted prepositional phrase whereas in experiment 1, the intervener consisted of a short RC followed by a PP. One can speculate as to why experiment  2 shows equally strong distance effects in both predictability conditions: processing a single long intervening phrase may be harder than processing two different phrases because it may be harder to chunk a single long phrase compared to two shorter phrases; this is predicted by the Sausage Machine proposal of  \citet{frazier1978sausage}. If this is correct, then the complexity of the intervener may indeed be a relevant factor in determining whether strong expectation can weaken locality effects. It is possible to test this claim by using an intervener that is much easier to process; an example would be an adverb containing no noun phrases. 

We were motivated by the recent replication crisis in psychology \citep{open2015estimating}  to attempt to replicate our results using a different method. Furthermore, replications using eye-tracking would be very informative because it is possible that self-paced reading overburdens the working-memory system in an unnatural manner. If this is the case, one prediction would be that the eye-tracking data would not necessarily show locality effects.
We describe these experiments next.

\section{Experiment 3}

\subsection{Method}
\subsubsection{Participants}

Forty participants, with the same criteria for inclusion as in the previous experiments, participated in the eye-tracking study in University of Potsdam, Germany. 

\subsubsection{Materials}

The experimental items were exactly the same as experiment 1 (self-paced reading), except that the 
following four items from experiment 1 were removed: item id 5, \textit{sheka:yat kardan} (complain + to do), item id 9, \textit{sahm bordan} (share + to win), item id 26, \textit{pishraft kardan} (progress + to do), and item id 32, \textit{hes kardan} (feel + to do). The reason for removal was that the results of the sentence-completion studies suggested that these light verbs had lower predictability than the other light verbs in the stimuli. It could be that this lower predictability is due to the existence of some other alternative light verbs with which the nominal part can combine to make other possible complex predicates. The last two CPs also had a lower acceptability rating (item 26 had 4.7, and item 32 had 3.5). As a consequence, in our eye-tracking study, we had thirty-two experimental items and sixty-four fillers. All items, including fillers are available in the supplementary materials.

\subsubsection{Procedure}

An eye-tracking study was prepared using Experiment-Builder software, and participants' eye-movements were recorded using an EyeLink 1000 tracker, with a connection to a PC. Before the experiment started, the participants were instructed to read the sentences silently at a normal pace and had a practice block consisting of five sentences. After answering the comprehension questions of the practice block, they were provided with feedback indicating whether or not the answer was correct. A 21-inch monitor was placed 60 centimeters from the participants' eyes. In order to reduce head movements, the participants were asked to use the chin-rest. They viewed the sentences with both eyes, but only the right eye was recorded. The items were presented in one line and in 18 points Persian Arial font (from right to left). First, they had to fixate on a dot at the right edge of the screen so that the sentence appeared. After they finished reading, they had to fixate on the dot in the bottom left corner of the screen; once they fixated on the dot,  the comprehension question was presented. Unlike the practice items, they were not provided with any feedback. Calibration was performed at the beginning of the experiment, after their 5-minute break (which occurred after they had were halfway through the experiment), and whenever it was necessary.

\subsubsection{Data analysis}
Raw gaze duration data was obtained using the Data Viewer software.\footnote{http://www.sr-research.com/dv.html} This data was then processed to get different eye-tracking measures using the \texttt{em2} package \citep{logacev2014}. As discussed earlier, Bayesian linear mixed models  were used for the analysis. All analyses were carried out using log-transformed data.  Zero ms reading times were removed before carrying out the analysis.

<<exp3analysis,echo=FALSE>>=
data1<-read.table("ET/ET1-final.txt", header=T)

# details of data 1 :
#head(data1)
#summary(data1)
#str(data1)
#length(unique(data1$subject))   # 40
#length(unique(data1$id))   # 32

data1$subject<-factor(data1$subject)
#data1$RESPONSE_ACCURACY<-factor(data1$RESPONSE_ACCURACY)
data1$cond<-factor(data1$cond)

#contrast coding :
## short -1, long 1
data1$dist<-ifelse(data1$cond%in%c("a","c"),-1,1)
## pred 1, -1
data1$pred<-ifelse(data1$cond%in%c("a","b"),1,-1)

#nested
data1$pred.dist <- ifelse(data1$cond=="a",-1,
                          ifelse(data1$cond=="b",1,0))
data1$nopred.dist <- ifelse(data1$cond=="c",-1,
                            ifelse(data1$cond=="d",1,0))

#Define the position column:

data1$position<-NA

# crit :
data1$position[data1$cond=="a" & data1$roi==4] <- "crit"
data1$position[data1$cond=="b" & data1$roi==5]<- "crit"
data1$position[data1$cond=="c" & data1$roi==4]<- "crit"
data1$position[data1$cond=="d" & data1$roi==5]<- "crit"

#preverb or object :
data1$position[data1$cond=="a" & data1$roi==2] <- "dep"
data1$position[data1$cond=="b" & data1$roi==2]<- "dep"
data1$position[data1$cond=="c" & data1$roi==2]<- "dep"
data1$position[data1$cond=="d" & data1$roi==2]<- "dep"

# subject :
data1$position[data1$cond=="a" & data1$roi==1] <- "predep"
data1$position[data1$cond=="b" & data1$roi==1]<- "predep"
data1$position[data1$cond=="c" & data1$roi==1]<- "predep"
data1$position[data1$cond=="d" & data1$roi==1]<- "predep"

# intervener :
data1$position[data1$cond=="a" & data1$roi==3] <- "intervener"
data1$position[data1$cond=="b" & data1$roi==3]<- "intervener"
data1$position[data1$cond=="b" & data1$roi==4]<- "intervener"
data1$position[data1$cond=="c" & data1$roi==3]<- "intervener"
data1$position[data1$cond=="d" & data1$roi==3]<- "intervener"
data1$position[data1$cond=="d" & data1$roi==4]<- "intervener"

#post crit
data1$position[data1$cond=="a" & data1$roi==5] <- "postcrit"
data1$position[data1$cond=="b" & data1$roi==6]<- "postcrit"
data1$position[data1$cond=="c" & data1$roi==5]<- "postcrit"
data1$position[data1$cond=="d" & data1$roi==6]<- "postcrit"

#post :
data1$position[data1$cond=="a" & data1$roi==6] <- "post"
data1$position[data1$cond=="b" & data1$roi==7]<- "post"
data1$position[data1$cond=="c" & data1$roi==6]<- "post"
data1$position[data1$cond=="d" & data1$roi==7]<- "post"

#Looking at the proportions of 0 fixations:
## counts of 0 ms fixations:
#nrow(data1[data1$FFD == 0&data1$cond=='a',])
#nrow(data1[data1$FFD == 0&data1$cond=='b',])
#nrow(data1[data1$FFD == 0&data1$cond=='c',])
#nrow(data1[data1$FFD == 0&data1$cond=='d',])

## 0 ms fixations:
#nrow(data1[data1$FFD == 0&data1$cond=='a',])/nrow(data1)
#nrow(data1[data1$FFD == 0&data1$cond=='b',])/nrow(data1)
#nrow(data1[data1$FFD == 0&data1$cond=='c',])/nrow(data1)
#nrow(data1[data1$FFD == 0&data1$cond=='d',])/nrow(data1)
#nrow(data1[data1$FFD == 0,])/nrow(data1)

#Data analysis of FPRT, RPD:

#contrast coding :
## short -1, long 1
data1$dist<-ifelse(data1$cond%in%c("a","c"),-1,1)
## pred 1, -1
data1$pred<-ifelse(data1$cond%in%c("a","b"),1,-1)

#nested
data1$pred.dist <- ifelse(data1$cond=="a",-1,
                          ifelse(data1$cond=="b",1,0))
data1$nopred.dist <- ifelse(data1$cond=="c",-1,
                            ifelse(data1$cond=="d",1,0))

## needed later for entropy analysis
etdata1<-data1

ET1.FPRT<-lmer(log(FPRT)~dist*pred+(1+dist*pred|subject)+(1+dist*pred|id),subset(data1,position=="crit" & FPRT!=0))
#summary(ET1.FPRT)
#summary(rePCA(ET1.FPRT))

## subj and obj top 3
#ET1.FPRTfin<-lmer(log(FPRT)~dist*pred+(1+dist+pred||subject)+(1+dist+pred||id),subset(data1,position=="crit" & FPRT!=0))
#summary(ET1.FPRTfin)
#qqPlot(residuals(ET1.FPRTfin))

#ET1.RPD<-lmer(log(RPD)~dist*pred+(1+dist*pred||subject)+(1+dist*pred||id),subset(data1,position=="crit" & RPD!=0))
#summary(ET1.RPD)
#summary(rePCA(ET1.RPD))
## subj and obj top 3
#ET1.RPDfin<-lmer(log(RPD)~dist*pred+(1+dist+pred||subject)+(1+dist+pred||id),subset(data1,position=="crit" & RPD!=0))
#summary(ET1.RPDfin)
#qqPlot(residuals(ET1.RPDfin))

if(0){
stanm3fprt<-stan_lmer(log(FPRT)~dist*pred+
                        (1+dist*pred|subject)+
               (1+dist*pred|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data1,position=="crit" & FPRT!=0))

save(stanm3fprt,file="stanm3fprt.Rda")

stanm3nestedfprt<-stan_lmer(log(FPRT)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist|subject)+
               (1+pred+pred.dist+nopred.dist|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data1,position=="crit" & FPRT!=0))

save(stanm3nestedfprt,file="stanm3nestedfprt.Rda")

## RPD:
stanm3rpd<-stan_lmer(log(RPD)~dist*pred+
                        (1+dist*pred|subject)+
               (1+dist*pred|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=3000,
                  cores=4,
                  data=subset(data1,position=="crit" & RPD!=0))

save(stanm3rpd,file="stanm3rpd.Rda")

stanm3nestedrpd<-stan_lmer(log(RPD)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist|subject)+
               (1+pred+pred.dist+nopred.dist|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=4000,
                  cores=4,
                  data=subset(data1,position=="crit" & RPD!=0))

save(stanm3nestedrpd,file="stanm3nestedrpd.Rda")

}

load("stanm3fprt.Rda")
load("stanm3nestedfprt.Rda")

load("stanm3rpd.Rda")
load("stanm3nestedrpd.Rda")

stanm3fprt_tab<-summary_stan_model(stanm3fprt)
stanm3nestedfprt_tab<-summary_stan_model(stanm3nestedfprt)

stanm3rpd_tab<-summary_stan_model(stanm3rpd)
stanm3nestedrpd_tab<-summary_stan_model(stanm3nestedrpd)
@

\subsection{Results}
\subsubsection{Comprehension accuracy}

<<exp3questionresponseanalysis,echo=FALSE>>=
etdata1q<-read.table("ET/etdata1qrespacc.txt",header=TRUE)
#head(etdata1q)

meanset1q<-round(with(etdata1q,tapply(resp,cond,mean))*100)

if(0){
stanglmeretqm1<-stan_glmer(resp~dist*pred+(1+dist+pred|subject)+
               (1+dist+pred|id),
               family=binomial(),
                     prior_intercept=student_t(df=2),
                     prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=3000,
                  cores=4,
                  data=etdata1q)

stanglmeretqm1nested<-stan_glmer(resp~pred+pred.dist+nopred.dist
                                 +(1+pred+pred.dist+nopred.dist|subject)+
               (1+pred+pred.dist+nopred.dist|id),
               family=binomial(),
                     prior_intercept=student_t(df=2),
                     prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=etdata1q)

save(stanglmeretqm1,file="stanglmeretqm1.Rda")

save(stanglmeretqm1nested,file="stanglmeretqm1nested.Rda")
}


load("stanglmeretqm1.Rda")
stanglmeretqm1_tab<-summary_stan_model(stanglmeretqm1)

load("stanglmeretqm1nested.Rda")
stanglmeretqm1nested_tab<-summary_stan_model(stanglmeretqm1nested)
@

On average, participants correctly answered  \Sexpr{round(mean(meanset1q))} percent of the target comprehension questions. Mean accuracy by condition was  
\Sexpr{round(meanset1q[1])}
percent  
for condition a, 
\Sexpr{round(meanset1q[2])}
percent for condition b, 
\Sexpr{round(meanset1q[3])}
percent for condition c, and 
\Sexpr{round(meanset1q[4])}
percent for condition d. We found no effects of 
distance and predictability, and no interaction.

\subsubsection{Reading time}

The critical region was the verb, as in Experiments 1 and 2.
The same sum contrast coding was used as in experiments 1 and 2; in addition, nested contrast coding was used to investigate the effect of distance within the two predictability conditions. 
We present results for first-pass reading time and regression path duration.

The effect of predictability, seen in Experiments 1 and 2, is also present in first-pass reading time (FPRT) and regression path duration (RPD); the strong-predictability conditions had shorter reading times.
There was also 
an effect of distance in FPRT but only a weak effect in RPD; the long-distance conditions had longer reading times. Table~\ref{Tab:03} shows the details of the analyses.
A nested contrast showed that in FPRT the distance 
effect was present in the weak-predictability conditions
(coef.=\Sexpr{stanm3nestedfprt_tab[4,2]},
[\Sexpr{stanm3nestedfprt_tab[4,3]},\Sexpr{stanm3nestedfprt_tab[4,4]}], and $P(b<0)$=\Sexpr{stanm3nestedfprt_tab[4,5]});
in the strong-predictability conditions the effect was weak
(coef.=\Sexpr{stanm3nestedfprt_tab[3,2]},
[\Sexpr{stanm3nestedfprt_tab[3,3]},\Sexpr{stanm3nestedfprt_tab[3,4]}], and $P(b<0)$=\Sexpr{stanm3nestedfprt_tab[3,5]}).
RPD showed only a weak effect of distance within the two predictability levels. For the weak-predictability level,
coef.=\Sexpr{stanm3nestedrpd_tab[4,2]},
[\Sexpr{stanm3nestedrpd_tab[4,3]},\Sexpr{stanm3nestedrpd_tab[4,4]}], $P(b<0)$=\Sexpr{stanm3nestedrpd_tab[4,5]}; and for the strong-predictability level,
coef.=\Sexpr{stanm3nestedrpd_tab[3,2]},
[\Sexpr{stanm3nestedrpd_tab[3,3]},\Sexpr{stanm3nestedrpd_tab[3,4]}], $P(b<0)$=\Sexpr{stanm3nestedrpd_tab[3,5]}.

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{Tab:03} Table \arabic{table}. }{Means, 95\% uncertainty intervals, and $P(b<0)$, the probability of the estimate being less than 0, in the reading time analysis for Experiment 3.}
\processtable{}
{\begin{tabular}{llrrrr}\toprule
ET measure & comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
FPRT & Intercept  & \Sexpr{stanm3fprt_tab[1,2]} & \Sexpr{stanm3fprt_tab[1,3]} & \Sexpr{stanm3fprt_tab[1,4]} & \Sexpr{stanm3fprt_tab[1,5]}\\
 & Distance  & \Sexpr{stanm3fprt_tab[2,2]} & \Sexpr{stanm3fprt_tab[2,3]} & \Sexpr{stanm3fprt_tab[2,4]} & \Sexpr{stanm3fprt_tab[2,5]}\\
 & Predictability  & \Sexpr{stanm3fprt_tab[3,2]} & \Sexpr{stanm3fprt_tab[3,3]} & \Sexpr{stanm3fprt_tab[3,4]} & \Sexpr{stanm3fprt_tab[3,5]}\\
 & Distance x Predictability  & \Sexpr{stanm3fprt_tab[4,2]} & \Sexpr{stanm3fprt_tab[4,3]} & \Sexpr{stanm3fprt_tab[4,4]} & \Sexpr{stanm3fprt_tab[4,5]}\\
RPD & Intercept  & \Sexpr{stanm3rpd_tab[1,2]} & \Sexpr{stanm3rpd_tab[1,3]} & \Sexpr{stanm3rpd_tab[1,4]} & \Sexpr{stanm3rpd_tab[1,5]}\\
 & Distance  & \Sexpr{stanm3rpd_tab[2,2]} & \Sexpr{stanm3rpd_tab[2,3]} & \Sexpr{stanm3rpd_tab[2,4]} & \Sexpr{stanm3rpd_tab[2,5]}\\
 & Predictability  & \Sexpr{stanm3rpd_tab[3,2]} & \Sexpr{stanm3rpd_tab[3,3]} & \Sexpr{stanm3rpd_tab[3,4]} & \Sexpr{stanm3rpd_tab[3,5]}\\
 & Distance x Predictability  & \Sexpr{stanm3rpd_tab[4,2]} & \Sexpr{stanm3rpd_tab[4,3]} & \Sexpr{stanm3rpd_tab[4,4]} & \Sexpr{stanm3rpd_tab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}


\begin{figure}[!htbp]
\begin{center}
<<plotsET1,echo=FALSE>>=
data1 <- na.omit(data1) 

data1<-subset(data1,FPRT!=0 & RPD!=0)
a.regions<-c("predep","dep","intervener","crit","postcrit", "post")
b.regions<-c("predep","dep","intervener","crit","postcrit", "post")
c.regions<-c("predep","dep","intervener","crit","postcrit", "post")
d.regions<-c("predep","dep","intervener","crit","postcrit", "post")

regions<-c(a.regions,b.regions,c.regions,d.regions)
region.id<-rep(1:6,4)
cond.id<-rep(letters[1:4],each=6)

region.df<-data.frame(cond=factor(cond.id),
                      region.id=region.id,
                      position=factor(regions))

data1.uniq.reg<-ddply(data1, 
                      .(subject, id, cond, position),
                      summarize,
                      FPRT = FPRT,
                      RPD = RPD)

data1.merged<-merge(data1.uniq.reg,region.df, 
                    by.x=c("cond","position"))
#summary(data1.merged)

data1 <- data1.merged

pred<-factor(ifelse(data1$cond%in%c("a","b"),
                    "pred","nopred"),
             levels=c("pred","nopred"))
dist<-factor(ifelse(data1$cond%in%c("a","c"),
                    "short","long"),
             levels=c("short","long"))

data1$pred<-pred
data1$dist<-dist

dataFPRT.rs <- melt(data1, 
                id=c("pred","dist","cond","position",
                     "subject","region.id"), 
                measure=c("FPRT"),
                na.rm=TRUE)

dataRPD.rs <- melt(data1, 
                id=c("pred","dist","cond","position",
                     "subject","region.id"), 
                measure=c("RPD"),
                na.rm=TRUE)

#dataTFT.rs <- melt(data1, 
#                id=c("pred","dist","cond","position",
#                     "subject","region.id"), 
#                measure=c("TFT"),
#                na.rm=TRUE)

## First-pass reading time

#get mean TFT and no. of data points for each region, for each subject/condition
dataFPRT.id <- data.frame(cast(dataFPRT.rs,
            subject+pred+dist+cond+position+region.id ~ ., 
                            function(x) c(FPRT=mean(x), N=length(x))))

#mean of mean rt of each subject
GM <- mean(tapply(dataFPRT.id$FPRT, dataFPRT.id$subject, mean))

#deviation from the grandmean after considering intra-subject variability
#removing between subject variance
dataFPRT.id <- ddply(dataFPRT.id, .(subject), 
                 transform, FPRT.w = FPRT - mean(FPRT) + GM)

temp<-melt(dataFPRT.id, id.var=c("subject","pred","dist","cond","position","region.id"), 
           measure.var="FPRT.w")

M.id.w <- cast(temp, pred+dist+cond+position+region.id  ~ ., 
               function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

M.id.w.pred <- cast(temp, pred+position  ~ ., 
                    function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

M.id.w.cp<-subset(M.id.w,pred=="pred")
M.id.w.cp$pred<-droplevels(M.id.w.cp$pred)

M.id.w.sp<-subset(M.id.w,pred=="nopred")
M.id.w.sp$pred<-droplevels(M.id.w.sp$pred)

M.id.w.orig<-M.id.w

colnames(M.id.w)[1]<-"Predictability"
M.id.w$Predictability<-factor(ifelse(M.id.w$Predictability=="pred","strong","weak"))


k<-1

plotfprt<-ggplot(subset(M.id.w,position=="crit"), 
             aes(x=dist, y=M,group=Predictability)) + 
   geom_point(shape=21,fill="white",size=k*3) +
   geom_line(aes(linetype=Predictability),size=k) +
   geom_errorbar(aes(ymin=M-2*SE, ymax=M+2*SE),
                 width=.1,size=k)+
   xlab("Distance")+
   ylab("FPRT (ms)")+
   labs(title="FPRT at the Critical region [Verb]") +
   theme_bw() +
   labs(legend.position=c(.87, .6))

plotfprt<-plotfprt+theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))

M.id.w <- M.id.w.orig

## Regression path duration
#get mean TFT and no. of data points for each region, for each subject/condition
dataRPD.id <- data.frame(cast(dataRPD.rs,
            subject+pred+dist+cond+position+region.id ~ ., 
                            function(x) c(RPD=mean(x), N=length(x))))

#mean of mean rt of each subject
GM <- mean(tapply(dataRPD.id$RPD, dataRPD.id$subject, mean))

#deviation from the grandmean after considering intra-subject variability
#removing between subject variance
dataRPD.id <- ddply(dataRPD.id, .(subject), 
                 transform, RPD.w = RPD - mean(RPD) + GM)

temp<-melt(dataRPD.id, id.var=c("subject","pred","dist","cond","position","region.id"), 
           measure.var="RPD.w")

M.id.w <- cast(temp, pred+dist+cond+position+region.id  ~ ., 
               function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

M.id.w.pred <- cast(temp, pred+position  ~ ., 
                    function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

M.id.w.cp<-subset(M.id.w,pred=="pred")
M.id.w.cp$pred<-droplevels(M.id.w.cp$pred)

M.id.w.sp<-subset(M.id.w,pred=="nopred")
M.id.w.sp$pred<-droplevels(M.id.w.sp$pred)

k<-1

M.id.w.orig<-M.id.w

colnames(M.id.w)[1]<-"Predictability"
M.id.w$Predictability<-factor(ifelse(M.id.w$Predictability=="pred","strong","weak"))

plotrpd<-ggplot(subset(M.id.w,position=="crit"), 
             aes(x=dist, y=M,group=Predictability)) + 
   geom_point(shape=21,fill="white",size=k*3) +
   geom_line(aes(linetype=Predictability),size=k) +
   geom_errorbar(aes(ymin=M-2*SE, ymax=M+2*SE),
                 width=.1,size=k)+
   xlab("Distance")+
   ylab("RPD (ms)")+
   labs(title="RPD at the Critical region [Verb]") +
   theme_bw() +
   labs(legend.position=c(.87, .6))

plotrpd<-plotrpd+theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))

M.id.w <- M.id.w.orig

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
multiplot(plotfprt,plotrpd,cols=1)
@
\caption{First-pass reading time and regression path duration in Experiment 3 at the critical verb. Error bars show 95\% confidence intervals.}\label{fig:ET1interaction-TFT}
\end{center}
\end{figure}

\subsection{Discussion}

In the eye-tracking Experiment 3, we replicated the locality effects found in the Experiment 1 in first-pass reading time. Nested contrasts showed that the  locality effect appeared in weak-predictability conditions, which is similar to the result in Experiment 1. 
A main effect of predictability was found in FPRT and RPD, replicating the effect in Experiment 1.  

 Since we failed to find any interaction between predictability and distance, we cannot conclude, as \citet{husain2014strong} did,  that expectation effects can cancel locality effects. The locality effects are consistent with working memory accounts \citep{gibson2000dependency,lewis2005activation} and inconsistent with the distance-based predictions of the expectation account \citep{levy2008expectation}. As in the SPR experiments, we have evidence consistent with a version of the expectation account  that predicts that strong predictability conditions will be read faster than the weak predictability conditions.

In sum, the main result in experiment 3 is that we have replicated the locality effect and the facilitation due to strong predictability. 

\section{Experiment 4}
\subsection{Method}
\subsubsection{Participants}

Forty participants, with the same criteria as in the previous experiments, participated in the eye-tracking study in Golm campus, University of Potsdam, Germany.

\subsubsection{Materials}

The experimental items were exactly the same as experiment 2 (self-paced reading), but with 32 items (see the explanation for Experiment 3 regarding the four items that were removed). The experimental items were complemented with 64 filler sentences with varying syntactic structures (see Supplementary materials).

\subsubsection{Procedure and Data Analysis}

The procedure and data analysis were exactly the same as experiment 3 (eye-tracking).

<<exptET2analysis,echo=FALSE>>=
data2<-read.table("ET/ET2-final.txt", header=T)
#head(data2)
## for some weird reason this subject column is 
## labeled subj:
colnames(data2)[1]<-("subject")
#length(unique(data2$subject))
#length(unique(data2$id))

#contrast coding :
## short -1, long 1
data2$dist<-ifelse(data2$cond%in%c("a","c"),-1,1)
## pred 1, -1
data2$pred<-ifelse(data2$cond%in%c("a","b"),1,-1)

#nested
data2$pred.dist <- ifelse(data2$cond=="a",-1,
                          ifelse(data2$cond=="b",1,0))
data2$nopred.dist <- ifelse(data2$cond=="c",-1,
                            ifelse(data2$cond=="d",1,0))


#### creating the column "position" to define the regions according to position:

data2$position<-NA

# crit :
data2$position[data2$cond=="a" & data2$roi==4]<- "crit"
data2$position[data2$cond=="b" & data2$roi==4]<- "crit"
data2$position[data2$cond=="c" & data2$roi==4]<- "crit"
data2$position[data2$cond=="d" & data2$roi==4]<- "crit"
#data2$position


#preverb or object :
data2$position[data2$cond=="a" & data2$roi==2] <- "dep"
data2$position[data2$cond=="b" & data2$roi==2]<- "dep"
data2$position[data2$cond=="c" & data2$roi==2]<- "dep"
data2$position[data2$cond=="d" & data2$roi==2]<- "dep"

# subject :
data2$position[data2$cond=="a" & data2$roi==1] <- "predep"
data2$position[data2$cond=="b" & data2$roi==1]<- "predep"
data2$position[data2$cond=="c" & data2$roi==1]<- "predep"
data2$position[data2$cond=="d" & data2$roi==1]<- "predep"

# intervener :
data2$position[data2$cond=="a" & data2$roi==3] <- "intervener"
data2$position[data2$cond=="b" & data2$roi==3]<- "intervener"
data2$position[data2$cond=="c" & data2$roi==3]<- "intervener"
data2$position[data2$cond=="d" & data2$roi==3]<- "intervener"

#post crit
data2$position[data2$cond=="a" & data2$roi==5] <- "postcrit"
data2$position[data2$cond=="b" & data2$roi==5]<- "postcrit"
data2$position[data2$cond=="c" & data2$roi==5]<- "postcrit"
data2$position[data2$cond=="d" & data2$roi==5]<- "postcrit"

#post :
data2$position[data2$cond=="a" & data2$roi==6] <- "post"
data2$position[data2$cond=="b" & data2$roi==6]<- "post"
data2$position[data2$cond=="c" & data2$roi==6]<- "post"
data2$position[data2$cond=="d" & data2$roi==6]<- "post"

#data2$position

etdata2<-data2

data2 <- na.omit(data2)

#head(data2)

a.regions<-c("predep","dep","intervener","crit","postcrit", "post")
b.regions<-c("predep","dep","intervener","crit","postcrit", "post")
c.regions<-c("predep","dep","intervener","crit","postcrit", "post")
d.regions<-c("predep","dep","intervener","crit","postcrit", "post")

regions<-c(a.regions,b.regions,c.regions,d.regions)
region.id<-rep(1:6,4)
cond.id<-rep(letters[1:4],each=6)

region.df<-data.frame(cond=factor(cond.id),
                      region.id=region.id,
                      roi=factor(regions))

## Expt 4 data analysis

#contrast coding reminder:
## short -1, long 1
data2$dist<-ifelse(data2$cond%in%c("a","c"),-1,1)
## pred 1, -1
data2$pred<-ifelse(data2$cond%in%c("a","b"),1,-1)

#nested
data2$pred.dist <- ifelse(data2$cond=="a",-1,
                          ifelse(data2$cond=="b",1,0))
data2$nopred.dist <- ifelse(data2$cond=="c",-1,
                            ifelse(data2$cond=="d",1,0))

#ET2.FPRT<-lmer(log(FPRT)~dist*pred+(1+dist*pred||subject)+(1+dist*pred||id),subset(data2,position=="crit" & FPRT!=0))
#summary(ET2.FPRT)
## keep max
#qqPlot(residuals(ET2.FPRT))

#ET2.RPD<-lmer(log(RPD)~dist*pred+(1+dist*pred||subject)+(1+dist*pred||id),subset(data2,position=="crit" & RPD!=0))
#summary(ET2.RPD)
#summary(rePCA(ET2.RPD))
#ET2.RPDfin<-lmer(log(RPD)~dist*pred+(1|subject)+(1+dist*pred||id),subset(data2,position=="crit" & RPD!=0))

#summary(ET2.RPDfin)
#qqPlot(residuals(ET2.RPDfin))

#ET2.FPRTnest<-lmer(log(FPRT)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist||subject)+(1+pred+pred.dist+nopred.dist||id),subset(data2,position=="crit" & FPRT!=0))
#summary(ET2.FPRTnest)
#qqPlot(residuals(ET2.FPRTnest))

#ET2.RPDnest<-lmer(log(RPD)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist||subject)+(1+pred+pred.dist+nopred.dist||id),subset(data2,position=="crit" & RPD!=0))
#summary(ET2.RPDnest)

#ET2.RPDnestfin<-lmer(log(RPD)~pred+pred.dist+nopred.dist+(1|subject)+(1+pred+nopred.dist||id),subset(data2,position=="crit" & RPD!=0))
#summary(ET2.RPDnestfin)
#qqPlot(residuals(ET2.RPDnestfin))

if(0){
stanm4fprt<-stan_lmer(log(FPRT)~dist*pred+
                        (1+dist*pred|subject)+
               (1+dist*pred|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=subset(data2,position=="crit" & FPRT!=0))

save(stanm4fprt,file="stanm4fprt.Rda")

stanm4nestedfprt<-stan_lmer(log(FPRT)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist|subject)+
               (1+pred+pred.dist+nopred.dist|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=3000,
                  cores=4,
                  data=subset(data2,position=="crit" & FPRT!=0))

save(stanm4nestedfprt,file="stanm4nestedfprt.Rda")

## RPD:
stanm4rpd<-stan_lmer(log(RPD)~dist*pred+
                        (1+dist*pred|subject)+
               (1+dist*pred|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=3000,
                  cores=4,
                  data=subset(data2,position=="crit" & RPD!=0))

save(stanm4rpd,file="stanm4rpd.Rda")

stanm4nestedrpd<-stan_lmer(log(RPD)~pred+pred.dist+nopred.dist+(1+pred+pred.dist+nopred.dist|subject)+
               (1+pred+pred.dist+nopred.dist|id),
                  prior_intercept=normal(0,6),
                  prior=normal(0,1),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=4000,
                  cores=4,
                  data=subset(data2,position=="crit" & RPD!=0))

save(stanm4nestedrpd,file="stanm4nestedrpd.Rda")

}

load("stanm4fprt.Rda")
load("stanm4nestedfprt.Rda")

load("stanm4rpd.Rda")
load("stanm4nestedrpd.Rda")

stanm4fprt_tab<-summary_stan_model(stanm4fprt)
stanm4nestedfprt_tab<-summary_stan_model(stanm4nestedfprt)

stanm4rpd_tab<-summary_stan_model(stanm4rpd)
stanm4nestedrpd_tab<-summary_stan_model(stanm4nestedrpd)
@

\subsection{Results}
\subsubsection{Comprehension Accuracy}

<<exp4questionresponseanalysis,echo=FALSE>>=
etdata2q<-read.table("ET/etdata2qrespacc.txt",header=TRUE)
#head(etdata2q)

meanset2q<-round(with(etdata2q,tapply(resp,cond,mean))*100)

if(0){
stanglmeretqm2<-stan_glmer(resp~dist*pred+(1+dist+pred|subject)+
               (1+dist+pred|id),
               family=binomial(),
                     prior_intercept=student_t(df=2),
                     prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=3000,
                  cores=4,
                  data=etdata2q)

stanglmeretqm2nested<-stan_glmer(resp~pred+pred.dist+nopred.dist
                                 +(1+pred+pred.dist+nopred.dist|subject)+
               (1+pred+pred.dist+nopred.dist|id),
               family=binomial(),
                     prior_intercept=student_t(df=2),
                     prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4,
                  data=etdata2q)

save(stanglmeretqm2,file="stanglmeretqm2.Rda")

save(stanglmeretqm2nested,file="stanglmeretqm2nested.Rda")
}


load("stanglmeretqm2.Rda")
stanglmeretqm2_tab<-summary_stan_model(stanglmeretqm2)

load("stanglmeretqm2nested.Rda")
stanglmeretqm2nested_tab<-summary_stan_model(stanglmeretqm2nested)
@

On average, participants answered \Sexpr{round(mean(meanset2q))} percent of  comprehension questions correctly. They had 
\Sexpr{round(meanset2q[1])}
percent response accuracy for condition a, 
\Sexpr{round(meanset2q[2])}
percent for condition b, 
\Sexpr{round(meanset2q[3])}
percent for condition c, and 
\Sexpr{round(meanset2q[4])}
percent for condition d. None of the factors had 
an effect on accuracy.

\subsubsection{Eye-tracking measures}

The reading times at the critical region are summarized in 
Figure~\ref{fig:ET2plot}.
Unlike experiment 3, in the current experiment, we found effects of distance and predictability in both the measures (see Table~\ref{Tab:04}). In other words, in the two measures reported, the long conditions (b and d) were read slower than the short conditions (a and c), and the weak predictability conditions (c and d) were read slower than the strong predictability conditions (a and b). None of the measures showed any interaction between predictability and distance.

Nested comparisons showed that in first-pass reading time, the locality effect was seen in the strong-predictability condition 
(coef.=\Sexpr{stanm4nestedfprt_tab[3,2]},
[\Sexpr{stanm4nestedfprt_tab[3,3]},
\Sexpr{stanm4nestedfprt_tab[3,4]}],
$P(b<0)$= \Sexpr{stanm4nestedfprt_tab[3,5]}),
but there was a weaker tendency towards a locality effect in the weak-predictability condition 
(coef.=\Sexpr{stanm4nestedfprt_tab[4,2]},
[\Sexpr{stanm4nestedfprt_tab[4,3]},
\Sexpr{stanm4nestedfprt_tab[4,4]}],
$P(b<0)$= \Sexpr{stanm4nestedfprt_tab[4,5]}).
In regression-path duration, both strong- and weak-predictability conditions showed a locality effect (strong-predictability: 
coef.=\Sexpr{stanm4nestedrpd_tab[3,2]},
[\Sexpr{stanm4nestedrpd_tab[3,3]},
\Sexpr{stanm4nestedrpd_tab[3,4]}],
$P(b<0)$= \Sexpr{stanm4nestedrpd_tab[3,5]};
low-predictability: 
coef.=\Sexpr{stanm4nestedrpd_tab[4,2]},
[\Sexpr{stanm4nestedrpd_tab[4,3]},
\Sexpr{stanm4nestedrpd_tab[4,4]}],
$P(b<0)$= \Sexpr{stanm4nestedrpd_tab[4,5]}.

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{Tab:04} Table \arabic{table}. }{Means, 95\% uncertainty intervals, and $P(b<0)$, the probability of the estimate being less than 0, in the reading time analysis for Experiment 4.}
\processtable{}
{\begin{tabular}{llrrrr}\toprule
ET measure & comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
FPRT & Intercept  & \Sexpr{stanm4fprt_tab[1,2]} & \Sexpr{stanm4fprt_tab[1,3]} & \Sexpr{stanm4fprt_tab[1,4]} & \Sexpr{stanm4fprt_tab[1,5]}\\
 & Distance  & \Sexpr{stanm4fprt_tab[2,2]} & \Sexpr{stanm4fprt_tab[2,3]} & \Sexpr{stanm4fprt_tab[2,4]} & \Sexpr{stanm4fprt_tab[2,5]}\\
 & Predictability  & \Sexpr{stanm4fprt_tab[3,2]} & \Sexpr{stanm4fprt_tab[3,3]} & \Sexpr{stanm4fprt_tab[3,4]} & \Sexpr{stanm4fprt_tab[3,5]}\\
 & Distance x Predictability  & \Sexpr{stanm4fprt_tab[4,2]} & \Sexpr{stanm4fprt_tab[4,3]} & \Sexpr{stanm4fprt_tab[4,4]} & \Sexpr{stanm4fprt_tab[4,5]}\\
RPD & Intercept  & \Sexpr{stanm4rpd_tab[1,2]} & \Sexpr{stanm4rpd_tab[1,3]} & \Sexpr{stanm4rpd_tab[1,4]} & \Sexpr{stanm4rpd_tab[1,5]}\\
 & Distance  & \Sexpr{stanm4rpd_tab[2,2]} & \Sexpr{stanm4rpd_tab[2,3]} & \Sexpr{stanm4rpd_tab[2,4]} & \Sexpr{stanm4rpd_tab[2,5]}\\
 & Predictability  & \Sexpr{stanm4rpd_tab[3,2]} & \Sexpr{stanm4rpd_tab[3,3]} & \Sexpr{stanm4rpd_tab[3,4]} & \Sexpr{stanm4rpd_tab[3,5]}\\
 & Distance x Predictability  & \Sexpr{stanm4rpd_tab[4,2]} & \Sexpr{stanm4rpd_tab[4,3]} & \Sexpr{stanm4rpd_tab[4,4]} & \Sexpr{stanm4rpd_tab[4,5]}\\
\botrule
\end{tabular}}{}
\end{table}



\begin{figure}[!htbp]
\begin{center}
<<plotexp4,echo=FALSE>>=
## Plots
pred<-factor(ifelse(data2$cond%in%c("a","b"),"pred",
                    "nopred"),
             levels=c("pred","nopred"))
dist<-factor(ifelse(data2$cond%in%c("a","c"),"short",
                    "long"),
             levels=c("short","long"))

data2$pred<-pred
data2$dist<-dist

data2.uniq.reg<-ddply(data2, 
                      .(subject, id, cond, position),
                      summarize,
                      FPRT = FPRT,
                      RPD = RPD)

dataFPRT.rs <- melt(data2, 
                id=c("pred","dist","cond","position","subject"), 
                measure=c("FPRT"),
                na.rm=TRUE)

dataRPD.rs <- melt(data2, 
                id=c("pred","dist","cond","position","subject"), 
                measure=c("RPD"),
                na.rm=TRUE)

#get mean and no. of data points for each region, for each subject/condition
dataFPRT.id  <- data.frame(cast(dataFPRT.rs, 
                            subject+pred+dist+cond+position ~ ., 
                            function(x) c(FPRT=mean(x), N=length(x) ) ))

dataRPD.id  <- data.frame(cast(dataRPD.rs, 
                            subject+pred+dist+cond+position ~ ., 
                            function(x) c(RPD=mean(x), N=length(x) ) ))

## First-pass reading time
GM <- mean(tapply(dataFPRT.id$FPRT, dataFPRT.id$subject, mean))

#deviation from the grandmean after considering intra-subject variability
#removing between subject variance
dataFPRT.id <- ddply(dataFPRT.id, .(subject), 
                 transform, FPRT.w = FPRT - mean(FPRT) + GM)

temp<-melt(dataFPRT.id, id.var=c("subject","pred","dist","cond","position"), 
           measure.var="FPRT.w")

M.id.w <- cast(temp, pred+dist+cond+position  ~ ., 
               function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

M.id.w.pred <- cast(temp, pred+position  ~ ., 
                    function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

k<-1

colnames(M.id.w)[1]<-"Predictability"
M.id.w$Predictability<-factor(ifelse(M.id.w$Predictability=="pred","strong","weak"))


pFPRT<-ggplot(subset(M.id.w,position=="crit"), 
             aes(x=dist, y=M,group=Predictability)) + 
   geom_point(shape=21,fill="white",size=k*3) +
   geom_line(aes(linetype=Predictability),size=k) +
   geom_errorbar(aes(ymin=M-2*SE, ymax=M+2*SE),
                 width=.1,size=k)+
   xlab("Distance")+
   ylab("FPRT (ms)")+
   labs(title="FPRT at the Critical region [Verb]") +
   theme_bw() +
   labs(legend.position=c(.87, .6))

pFPRT<-pFPRT+theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))

M.id.w <- M.id.w.orig

## Regression-path duration
GM <- mean(tapply(dataRPD.id$RPD, dataRPD.id$subject, mean))

#deviation from the grandmean after considering intra-subject variability
#removing between subject variance
dataRPD.id <- ddply(dataRPD.id, .(subject), 
                 transform, RPD.w = RPD - mean(RPD) + GM)

temp<-melt(dataRPD.id, id.var=c("subject","pred","dist","cond","position"), 
           measure.var="RPD.w")

M.id.w <- cast(temp, pred+dist+cond+position  ~ ., 
               function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

M.id.w.pred <- cast(temp, pred+position  ~ ., 
                    function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x) ) )

k<-1

colnames(M.id.w)[1]<-"Predictability"
M.id.w$Predictability<-factor(ifelse(M.id.w$Predictability=="pred","strong","weak"))

pRPD<-ggplot(subset(M.id.w,position=="crit"), 
             aes(x=dist, y=M,group=Predictability)) + 
   geom_point(shape=21,fill="white",size=k*3) +
   geom_line(aes(linetype=Predictability),size=k) +
   geom_errorbar(aes(ymin=M-2*SE, ymax=M+2*SE),
                 width=.1,size=k)+
   xlab("Distance")+
   ylab("RPD (ms)")+
   labs(title="RPD at the Critical region [Verb]") +
   theme_bw() +
   labs(legend.position=c(.87, .6))

pRPD<-pRPD+theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))
M.id.w <- M.id.w.orig

multiplot(pFPRT,pRPD,cols=1)
@
\caption{First-pass reading time and regression path duration in Experiment 4 at the critical verb. Error bars show 95\% confidence intervals.}\label{fig:ET2plot}
\end{center}
\end{figure}

\subsection{Discussion}
Experiment 4 replicated the results of experiment 2: a main effect of distance and a main effect of predictability, with no evidence for an interaction. The effects in FPRT and RPD showed essentially the same patterns as in the first eye-tracking study. However, the locality effects were even stronger, in the same way that the second self-paced reading study showed stronger locality effects. Also, these effects are equally strong in both strong and weak predictability conditions, mirroring our finding in the second self-paced reading study. 

Overall, regarding the distance manipulation, the results are consistent with memory-based accounts, and inconsistent with the expectation account. The main effect of predictability is consistent with the expectation account, as discussed earlier.
In Experiment 4, we don't see any evidence consistent with the \citet{husain2014strong} proposal; if anything, the locality effect is \textit{stronger} in the strong-predictability conditions. 

\section{General discussion}

\begin{figure}[!htbp]
\begin{center}
<<summarizeresults,echo=FALSE>>=
## prepare summary
stanm1_tab<-stanm1_tab[2:4,]
stanm2_tab<-stanm2_tab[2:4,]
stanm3fprt_tab<-stanm3fprt_tab[2:4,]
stanm3rpd_tab<-stanm3rpd_tab[2:4,]
stanm4fprt_tab<-stanm4fprt_tab[2:4,]
stanm4rpd_tab<-stanm4rpd_tab[2:4,]

stanm1_tab$experiment<-"e1"
stanm2_tab$experiment<-"e2"
stanm3fprt_tab$experiment<-"e3"
stanm3rpd_tab$experiment<-"e3"
stanm4fprt_tab$experiment<-"e4"
stanm4rpd_tab$experiment<-"e4"
stanm1_tab<-as.data.frame(stanm1_tab[c(6,1:4)])
stanm2_tab<-as.data.frame(stanm2_tab[c(6,1:4)])
stanm3fprt_tab<-as.data.frame(stanm3fprt_tab[c(6,1:4)])
stanm3rpd_tab<-as.data.frame(stanm3rpd_tab[c(6,1:4)])
stanm4fprt_tab<-as.data.frame(stanm4fprt_tab[c(6,1:4)])
stanm4rpd_tab<-as.data.frame(stanm4rpd_tab[c(6,1:4)])

sprdata<-rbind(stanm1_tab,stanm2_tab)
etdatafprt<-rbind(stanm3fprt_tab,stanm4fprt_tab)
etdatarpd<-rbind(stanm3rpd_tab,stanm4rpd_tab)

colnames(sprdata)[3]<-"estimate"
colnames(etdatafprt)[3]<-"estimate"
colnames(etdatarpd)[3]<-"estimate"

sprdata$comparison<-factor(sprdata$comparison,
                           levels=c("dist","pred","dist:pred"))

etdatafprt$comparison<-factor(etdatafprt$comparison,
                           levels=c("dist","pred","dist:pred"))

etdatarpd$comparison<-factor(etdatarpd$comparison,
                           levels=c("dist","pred","dist:pred"))

#factor(sprdata$comparison)

sprdata$estimate<-as.numeric(as.character(sprdata$estimate))
sprdata$lower<-as.numeric(as.character(sprdata$lower))
sprdata$upper<-as.numeric(as.character(sprdata$upper))

etdatafprt$estimate<-as.numeric(as.character(etdatafprt$estimate))
etdatafprt$lower<-as.numeric(as.character(etdatafprt$lower))
etdatafprt$upper<-as.numeric(as.character(etdatafprt$upper))

etdatarpd$estimate<-as.numeric(as.character(etdatarpd$estimate))
etdatarpd$lower<-as.numeric(as.character(etdatarpd$lower))
etdatarpd$upper<-as.numeric(as.character(etdatarpd$upper))

#str(sprdata)

pd<-position_dodge(0.3)

decorate<-theme(axis.text=element_text(size=14),
        axis.title=element_text(size=14,face="bold"))+theme(legend.text = element_text(colour="black", size = 16, face = "bold"))+
  theme(legend.title = element_text(colour="black", size = 16, face = "bold"))

sprplot<-ggplot(sprdata, aes(x=comparison, 
                             y=estimate, colour=experiment, group=experiment)) +
        geom_errorbar(aes(ymin=lower, ymax=upper),
                      width=.2, size=0.25, colour="black", position=pd) +
       labs(title="Log RT") +
  geom_hline(yintercept=0)+
geom_point(position=pd, size=2.5)+theme_bw()

sprplot<-sprplot+decorate+ylim(-.1,.1)

## et data:
etplotfprt<-ggplot(etdatafprt, aes(x=comparison, 
                             y=estimate, colour=experiment, group=experiment)) +
        geom_errorbar(aes(ymin=lower, ymax=upper),
                      width=.2, size=0.25, colour="black", position=pd) +
       labs(title="Log FPRT") +
  geom_hline(yintercept=0)+
geom_point(position=pd, size=2.5)+theme_bw()

etplotfprt<-etplotfprt+decorate

etplotrpd<-ggplot(etdatarpd, aes(x=comparison, 
                             y=estimate, colour=experiment, group=experiment)) +
        geom_errorbar(aes(ymin=lower, ymax=upper),
                      width=.2, size=0.25, colour="black", position=pd) +
       labs(title="Log RPD") +
  geom_hline(yintercept=0)+
geom_point(position=pd, size=2.5)+theme_bw()

etplotrpd<-etplotrpd+decorate

multiplot(sprplot,etplotfprt,etplotrpd,cols=1)
@
\caption{Summary of the magnitudes of effects (derived from the linear mixed models) across the four experiments. The error bars show 95\% uncertainty intervals and show the range within which we can be 95\% certain that the true parameter lies given the data.}\label{fig:all}
\end{center}
\end{figure}

As summarized graphically in Figure~\ref{fig:all}, 
our main finding from the four Persian studies is that the locality effect predicted by memory accounts is upheld, but there is no evidence for the expectation-based account's prediction of facilitation in longer distance conditions. 
We consistently see a main effect of predictability, which is consistent with expectation accounts.
Finally, there is no compelling evidence in the Persian data that strong expectations cancel locality effects.

There is also suggestive evidence that the complexity of intervening material could strengthen the locality effect: when the intervener is an RC followed by a PP, we see a marginal interaction between distance and predictability, but when the intervener is a   
single long PP, we see no evidence for an interaction between distance and predictability strength, and we tend to see stronger effects.  

We consistently found a main effect of predictability in all four experiments: the strong predictability conditions were read faster at the verb than the weak predictability conditions. This is consistent with the  expectation-based account. Since the verbs in the strong and weak predictability conditions are not identical, we cannot rule out the possibility that word frequency or other such low-level factors are responsible for these effects. However, it is plausible that the highly predictable verb is processed faster than the less predictable verb. Thus, the main effect of predictability can be seen as evidence for expectation-based accounts, operationalized in terms of the conditional probabilities of the appearance of the exact verb given the preceding context.
  
It is possible that we were unable to replicate Husain et al's findings because of the nature of the intervener used in the Persian studies. Unlike \cite{husain2014strong} where the long distance condition had extra adverbials compared to the short condition, in Experiment 1 we have a more complex intervener, a relative clause. Another reason for finding the effects which are different from the study by \cite{husain2014strong} could be that in Persian, separating the nominal part of the CP from the light verb occurs relatively rarely, compared to Hindi. There is some support for this in corpus data. 
Based on the Hindi dependency treebank \citep{bhatt2009multi}, the average distance, counted as the number of intervening phrases, between an object and its (heavy) verb is 0.82 (with minimum 0 and maximum 15, and first and third quantiles 0 and 1), and the average distance between a noun and light verb is .07 (minimum 0 and maximum 18, with first and third quantiles 0 and 0). In the Persian dependency treebank \citep{Mojganphd}, the average distance between an object and (heavy) verb is 2.48 (with minimum 0 and maximum 9, and first and third quantiles 1 and 3), while the average distance between a noun and light verb is 0.05 (with minimum 0, and maximum 6, and first and third quantiles 0 and 0). Thus, the adjacency of CPs in Persian is strongly preferred (maximum 6 vs.\ Hindi's maximum 18), although as validated in the acceptability rating norming study, this separability is apparently acceptable and not considered ungrammatical.\footnote{These intervening phrases have been computed using dependency treebanks. Consequently, phrasal boundaries are approximations. Also, because of annotation differences between the two treebanks, phrase boundary criteria sometimes differ for the two languages. The phrasal counts lead to the same conclusions regardless of whether one counts intervening phrases or words.}

\subsection{An alternative explanation of locality effects in terms of entropy}

<<entropycode,echo=FALSE>>=
d<-read.table('Pretests/entropy/sentcomp.txt', header=T)


#subset experiment 1 data
exp1<-subset(d, Pre.test==1)
exp1$subj.ID<-factor(exp1$subj.ID)

#xtabs(~subj.ID+cond,exp1)
## H, N, M are duplicated, mistake in assigning unique ids
## fixed below:
exp1$subj.ID<-paste(exp1$LatSq.list,exp1$subj.ID,sep="")
#xtabs(~subj.ID+cond,exp1)

library(plyr)
temp<-ddply(exp1,.(item.ID,cond,verb),nrow)
#head(temp)

#in three instances total count !=8
#1b     7
#3a     7
#14b    7
temp1<-ddply(temp,.(item.ID,cond),summarize,count=sum(V1))
#head(temp1)

exp1.verbcount<-merge(temp, temp1, by.x=c("item.ID", "cond"))

exp1.verbcount$prob <- exp1.verbcount$V1/exp1.verbcount$count
#head(exp1.verbcount)

exp1.verbcount[] <- lapply(exp1.verbcount, as.character)

## code rechecked by SV 21 Feb 2016 (correct)
for(i in 1:length(unique(exp1.verbcount$item.ID))) #iterate over each item
{
  cond <- unique(exp1.verbcount$cond)
  
  for(j in 1:length(cond)) #iterate over each cond
  {
    #get number of unique verbs of an item and condition
    verbs <- length(exp1.verbcount[exp1.verbcount$item.ID==i&exp1.verbcount$cond==cond[j],]$prob)
    
    sum = 0
    #compute entropy
    for(k in 1:verbs)
    {
      sum = sum + (as.numeric(exp1.verbcount[exp1.verbcount$item.ID==i&exp1.verbcount$cond==cond[j],]$prob[k])
                   *log2(as.numeric(exp1.verbcount[exp1.verbcount$item.ID==i&exp1.verbcount$cond==cond[j],]$prob[k])))
    }
    
    exp1.verbcount$entropy[exp1.verbcount$item.ID==i&exp1.verbcount$cond==cond[j]] <- -1 * sum
  }
}

#head(exp1.verbcount)

exp1.frame<-ddply(exp1.verbcount,.(item.ID,cond,count,entropy), nrow)
colnames(exp1.frame)[5] <- "unique.verbs"
#head(exp1.frame)

exp1.frame$cond<-factor(exp1.frame$cond)

#entropy
means<-with(exp1.frame,tapply(entropy,cond,mean))
stddev<-with(exp1.frame,tapply(entropy,cond,sd))
stderr<-stddev/sqrt(36)

## entropy plot for paper:
Distance<-factor(c(rep(c("short","long"),2)),levels=c("short","long"))
Predicate_Type<-factor(c(rep(c("complex","simple"),each=2)),levels=c("complex","simple"))

entropydate1<-data.frame(Predicate_Type=Predicate_Type,Distance=Distance,ent=means,se=stderr)

#library(ggplot2)

entp1<-ggplot(entropydate1, aes(x=Predicate_Type, y=ent, fill=Distance)) +
  geom_bar(position="dodge", stat="identity")+
  geom_errorbar(aes(ymin=ent-2*se, ymax=ent+2*se), 
                position=position_dodge(0.9),width=.2)+
  xlab("Predicate Type")+ylab("Entropy")+ggtitle("Intervener RC+PP (Expts 1,3)")+theme_bw()

## difference 1 lower than original calculation
#entp1

#====================
#experiment 2

exp2<-subset(d, Pre.test==2)
exp2$subj.ID<-factor(exp2$subj.ID)
#xtabs(~subj.ID+cond,exp2)

temp<-ddply(exp2,.(item.ID,cond,verb),nrow)
#head(temp)

#one instance of total count !=8
#15c     6
temp1<-ddply(temp,.(item.ID,cond),summarize,count=sum(V1))
#head(temp1)

exp2.verbcount<-merge(temp, temp1, by.x=c("item.ID", "cond"))

exp2.verbcount$prob <- exp2.verbcount$V1/exp2.verbcount$count
#head(exp2.verbcount)

exp2.verbcount[] <- lapply(exp2.verbcount, as.character)

for(i in 1:length(unique(exp2.verbcount$item.ID))) #iterate over each item
{
  cond <- unique(exp2.verbcount$cond)
  
  for(j in 1:length(cond)) #iterate over each cond
  {
    #get number of unique verbs of an item and condition
    verbs <- length(exp2.verbcount[exp2.verbcount$item.ID==i&exp2.verbcount$cond==cond[j],]$prob)
    
    sum = 0
    #compute entropy
    for(k in 1:verbs)
    {
      sum = sum + (as.numeric(exp2.verbcount[exp2.verbcount$item.ID==i&exp2.verbcount$cond==cond[j],]$prob[k])
                   *log2(as.numeric(exp2.verbcount[exp2.verbcount$item.ID==i&exp2.verbcount$cond==cond[j],]$prob[k])))
    }
    
    exp2.verbcount$entropy[exp2.verbcount$item.ID==i&exp2.verbcount$cond==cond[j]] <- -1 * sum
  }
}

exp2.frame<-ddply(exp2.verbcount,.(item.ID,cond,count,entropy), nrow)
colnames(exp2.frame)[5] <- "unique.verbs"
#head(exp2.frame)

exp2.frame$cond<-factor(exp2.frame$cond)

#entropy
means<-with(exp2.frame,tapply(entropy,cond,mean))
stddev<-with(exp2.frame,tapply(entropy,cond,sd))
stderr<-stddev/sqrt(36)

Distance<-factor(c(rep(c("short","long"),2)),levels=c("short",
                                                      "long"))
Predicate_Type<-factor(c(rep(c("complex","simple"),each=2)),levels=c("complex","simple"))

entropydate2<-data.frame(Predicate_Type=Predicate_Type,Distance=Distance,ent=means,se=stderr)

entp2<-ggplot(entropydate2, aes(x=Predicate_Type, y=ent, fill=Distance)) +
  geom_bar(position="dodge", stat="identity")+
  geom_errorbar(aes(ymin=ent-2*se, ymax=ent+2*se), 
                position=position_dodge(0.9),width=.2)+
  xlab("Predicate Type")+ylab("Entropy")+ggtitle("Intervener PP (Expts 2, 4)")+scale_y_continuous(limits = c(0, 2))+theme_bw()

#entp2

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
@


\begin{figure}[!htbp]
\begin{center}
<<entropyplot,echo=FALSE>>=
multiplot(entp1,entp2,cols=2)
@
\caption{The estimated entropy (with 95\% confidence intervals), computed using the sentence completion data, for the two experiment designs.}\label{fig:entropy}
\end{center}
\end{figure}


<<entropydiffse1e2,echo=FALSE>>=
#high predictibility conditions long vs short difference in entropy (Exp1 vs Exp2)
diffe1ba<-subset(exp1.frame,cond=="b")$entropy-subset(exp1.frame,cond=="a")$entropy
e1entdata<-data.frame(item.ID=subset(exp1.frame,cond=="b")$item.ID,diffent=diffe1ba,exp=factor("e1"))

diffe2ba<-subset(exp2.frame,cond=="b")$entropy-subset(exp2.frame,cond=="a")$entropy
e2entdata<-data.frame(item.ID=subset(exp2.frame,cond=="b")$item.ID,diffent=diffe2ba,exp=factor("e2"))

#t.test(diffe1ba,diffe2ba,paired=T)

e1e2entdata<-rbind(e1entdata,e2entdata)

e1e2entdata$experiment<-ifelse(e1e2entdata$exp=="e1",1,-1)
#summary(lmer(diffent~experiment+(1|item.ID),e1e2entdata))

## stan model
if(0){
diffente1e2stan<-stan_lmer(diffent~experiment+
                    (1+experiment|item.ID),
                  prior_intercept=student_t(df=2),
                  prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  e1e2entdata,
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4)

save(diffente1e2stan,file="diffente1e2stan.Rda")
}
load("diffente1e2stan.Rda")
diffente1e2stan_tab<-summary_stan_model(diffente1e2stan,nfixefs=1)
#str(diffente1e2stan_tab)
@

Could there be an alternative explanation for the locality effect seen in the four experiments, one that does not invoke greater memory cost in the long-distance conditions?  One possibility is that entropy (uncertainty) increases with increasing distance. Entropy is an information-theoretic measure that essentially represents how uncertain we are of the outcome \citep{shannon2001mathematical}. In the present case, this would translate to our uncertainty about the upcoming verb.  If there are $n$ possible ways to continue a sentence,  and each of the possible ways has probability $p_i$, where $i=1,\dots, n$, then entropy is defined \citep{shannon2001mathematical} as $-\sum_{i} p_i \times \log_2 (p_i)$.
The entropy associated with the upcoming verb can be calculated using our offline sentence completion data.\footnote{See \citet{linzen2015uncertainty} for a recent empirical investigation of entropy in sentence comprehension using corpus data instead of sentence completion data.  Linzen and Jaeger calculated entropy in several ways, and also evaluated another metric called entropy reduction (ER), which was proposed by \citealp{hale06}; however, we cannot evaluate ER here because that would require knowing the entropy for the word preceding the verb.} 

\subsubsection{Evaluating the effect of entropy}
In order to evaluate whether entropy could explain the locality data, we computed entropy for each item in each condition for both experiments. The estimated entropies for each condition in the two experiment designs are shown in Figure~\ref{fig:entropy}. 
It is important to note here that entropy for each condition in Figure~\ref{fig:entropy} is based on only nine data points per condition (we only have $9\times 4=36$ items); for different items, there is substantial variability in the entropy patterns by condition.
Nevertheless, in the figure we can see that in the items used for Experiments 1 and 3, the entropy is higher in the long-distance conditions. The effect of entropy is less clear for the items used in Experiments 2 and 4, because of the relatively wider confidence intervals. 
Clearly uncertainty is higher in the RC+PP experiment than in the long PP experiment. A closer look at the high predictability conditions shows that the entropy difference between the long and short distance conditions is larger in the RC+PP intervener items than the entropy difference in the long PP intervener items 
(it is larger by \Sexpr{round(as.numeric(as.character(diffente1e2stan_tab[2,2])),digits=2)}, with 95\% uncertainty intervals \Sexpr{round(as.numeric(as.character(diffente1e2stan_tab[2,3])),digits=2)} and \Sexpr{round(as.numeric(as.character(diffente1e2stan_tab[2,4])),digits=2)}, probability of the difference in entropy being less than 0 is \Sexpr{round(as.numeric(as.character(diffente1e2stan_tab[2,5])),digits=2)}).  This is suggestive---if weak---evidence that the intervening RC may be responsible for creating a greater degree of uncertainty regarding the upcoming verb.   This is a bit surprising because stronger locality effects were seen in the long PP experiments. 

<<entropyrtanalysis,echo=FALSE>>=
exp1entropy<-exp1.frame[,c(1,2,4)]
exp2entropy<-exp2.frame[,c(1,2,4)]

#SPR exp1 data
data1<-read.table("data_spr/Persiane1.txt", header=T)
data1$pos<-factor(data1$pos)
data1$resp<-factor(data1$resp)
data1$cond<-factor(data1$cond)
## short -1, long 1
data1$dist<-ifelse(data1$cond%in%c("a","c"),-1,1)
## pred 1, -1
data1$pred<-ifelse(data1$cond%in%c("a","b"),1,-1)

#nested
data1$pred.dist <- ifelse(data1$cond=="a",-1,
                          ifelse(data1$cond=="b",1,0))
data1$nopred.dist <- ifelse(data1$cond=="c",-1,
                            ifelse(data1$cond=="d",1,0))

## merge with data:
data1crit<-subset(data1,roi=="crit" & rt<3000)

data1critent<-merge(data1crit,exp1entropy,by.x=c("item","cond"),
                    by.y=c("item.ID","cond"))

## sanity check
#dim(data1crit)
#dim(data1critent)
#summary(data1critent)

## center:
data1critent$c_ent<-scale(data1critent$entropy,scale=F)

library(lme4)

m1ent<-lmer(log(rt)~pred*dist*c_ent+(1+dist+pred:dist||subj)+(1+c_ent||item),data1critent)
#summary(m1ent)

## stan:
if(0){
rtente1stan<-stan_lmer(log(rt)~pred*dist*c_ent+
                             +(1+pred*dist*c_ent|subj)+
                    (1+pred*dist*c_ent|item),
                  prior_intercept=student_t(df=2),
                  prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                data1critent,
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4)

save(rtente1stan,file="rtente1stan.Rda")
}

load("rtente1stan.Rda")
rtente1stan_tab<-summary_stan_model(rtente1stan,nfixefs=7)
@

\begin{table}[!htbp]
\textbf{\refstepcounter{table}\label{tab:entropyrtresult} Table \arabic{table}. }{Model results from the Bayesian linear mixed model for the effect of entropy (apart from other predictors) on log reading times in Experiment 1. Shown are the mean and 95\% uncertainty intervals, and the probability of the parameter being less than 0.}
\processtable{}
{\begin{tabular}{lrrrr}\toprule
%Comparison & Coefficient & SE & t-value \\
comparison & mean & lower & upper & $P(b<0)$ \\
\midrule   
Intercept & 
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[1,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[1,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[1,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[1,5])),digits=2)} \\
Predictability &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[2,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[2,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[2,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[2,5])),digits=2)} \\
Distance &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[3,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[3,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[3,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[3,5])),digits=2)} \\
Entropy &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[4,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[4,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[4,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[4,5])),digits=2)} \\
Pred:Dist &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[5,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[5,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[5,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[5,5])),digits=2)} \\
Pred:Entropy &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[6,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[6,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[6,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[6,5])),digits=2)} \\
Dist:Entropy &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[7,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[7,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[7,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[7,5])),digits=2)} \\
Pred:Dist:Ent &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[8,2])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[8,3])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[8,4])),digits=2)} &
\Sexpr{round(as.numeric(as.character(rtente1stan_tab[8,5])),digits=2)} \\
\botrule
\end{tabular}}{}
\end{table}

In order to investigate whether entropy affects reading times at the verb, 
we fit a maximal Bayesian linear mixed model with predicate type and distance as sum-coded factors, and entropy (centered) as a continuous predictor; all higher order interactions were also included. 
The dependent variable was log reading time at the critical verb. As shown in 
Table~\ref{tab:entropyrtresult},
in Experiment 1, in addition to the effects of predictability and distance, we find an effect of entropy, and an interaction between distance and entropy, such that long distance conditions lead to a greater effect of entropy. None of the other experiments showed any effects of entropy.
Thus, although the evidence in favor of entropy is far from overwhelming, 
a potentially important finding here is that entropy could explain locality effects at least in our experiment 1. To our knowledge, this is the first demonstration that locality effects may arise due to factors other than memory costs. 

<<grammaticalitysentcomp,echo=FALSE>>=
#sentence completion target/condition & grammaticality/condition analysis
exp1abo<-read.table("Pretests/entropy/exp1aborevised.txt",header=TRUE)
#xtabs(~subject+cond,exp1abo)

#grammaticality (experiment 1 a vs b)

#grammaticality in the long vs short condition in experiment 1
means<-with(exp1abo,tapply(gram,cond,mean))

## just checking:
#xtabs(~subject+condition,exp1abo)
#summary(exp1abo)
#xtabs(~subject+condition,exp1abo)
#xtabs(~subject+item.ID,exp1abo)
#xtabs(~item.ID+condition,exp1abo)
#xtabs(~item.ID+cond,exp1)
#sort(subset(exp1abo,subject=="A1")$item.ID)
#sort(subset(exp1abo,subject=="A1")$item.ID)

## doesn't converge:
#mglmergram<-glmer(gram~condition+
#                        (1|subject)+
#                        (1|item.ID),
#                  family=binomial(),
#                  exp1abo)

if(0){
mstangram<-stan_glmer(gram~condition+
                        (1+condition|subject)+
                        (1+condition|item.ID),
                      family=binomial(),
                      prior_intercept=student_t(df=2),
                      prior=student_t(df=2),
                      prior_covariance=decov(regularization=2),
                      exp1abo,
                      algorithm="sampling",
                      adapt_delta=0.9999,
                      chains=4,
                      iter=2000,
                      cores=4)

save(mstangram,file="mstangram.Rda")
}

load("mstangram.Rda")

#print(mstangram)

mstangram_tab<-summary_stan_model(mstangram,
                                  nfixefs=1)

logodds<-as.numeric(as.character(mstangram_tab[2,2]))
gramodds<-exp(logodds)
logoddslower<-as.numeric(as.character(mstangram_tab[2,3]))
gramoddslower<-exp(logoddslower)
logoddsupper<-as.numeric(as.character(mstangram_tab[2,4]))
gramoddsupper<-exp(logoddsupper)
probneg<-round(as.numeric(as.character(mstangram_tab[2,5])),digits=2)
@

But why does entropy increase in longer-distance dependencies? A possible explanation suggests itself in terms of memory overload causing forgetting. It is possible that the participants forgot that a noun-verb dependency exists in the long-distance complex predicate condition. One prediction of this forgetting-inducing-entropy
account would be that in the sentence completion study, participants would tend to produce more ungrammatical continuations in the long-distance condition than the short-distance condition. This is borne out in experiment 1: 
the accuracy in the short condition was 
\Sexpr{round(means[1],digits=2)*100} percent, and in the long condition it was \Sexpr{round(means[2],digits=2)*100} percent. A Bayesian generalized linear mixed model was fit with a full variance-covariance matrix for participants and items.\footnote{The predictor (short vs long condition) was coded using sum contrasts, with the long condition coded as $1$ and the short condition as $-1$; the dependent variable was binary and represented whether a target verb was produced by the participant for a particular item-condition combination or not. Participants and items were specified as partially crossed random factors, and a full variance-covariance matrix was fit for both random effects. The priors for the intercept and slope were the Student's t-distribution with 2 degrees of freedom, allowing a range of approximately $-10$ to $10$ on the log odds scale, with 0 the most likely value. The prior on the variance-covariance matrices was defined via the LKJ prior \citep{stan-software:2013,stan-manual:2014} on the correlation matrix; see \citet{SorensenVasishthTutorial} for a tutorial intended for psycholinguists and cognitive scientists.  The model was fit using the \texttt{stan\_lmer} function from the  \texttt{rstanrarm} package \citep{rstanarm2016}.} 
The results of the model fit showed a reduction in grammaticality of sentence completions in the long vs. short conditions; the log odds were \Sexpr{logodds} [\Sexpr{logoddslower}, \Sexpr{logoddsupper}],
%[-0.73, 0.09]$, 
with a probability of the log odds being negative being 
\Sexpr{probneg}.
%$0.93$.  
The sentence completion study corresponding to Experiment 2 (which showed no effect of entropy on reading times) showed no difference in grammaticality of completions; the short and long conditions had grammatical continuations with the proportions $0.97$ and $0.98$.

Thus, it is possible that in experiment 1, the increase in entropy is due to participants forgetting the left context partially.  Clearly, a planned experiment is called for to investigate this further. An important point to note here is that the increased entropy in the long-distance condition may be a \textit{consequence} of forgetting, not a cause in itself: entropy itself would not predict any increase in ungrammatical continuations, but the forgetting hypothesis does.

\subsubsection{Does predictability of an upcoming verb increase with distance?}

We showed above that increasing uncertainty about the upcoming verb may explain locality, at least in experiment 1. One important question that arises, especially in the strong predictability conditions, is the following: does increasing distance nevertheless sharpen the expectation for the verb, as suggested by  \citet{konieczny2000locality}? In order to address this question, 
we fit a Bayesian generalized linear mixed model (GLMM) with a logistic link function that investigated the change in probability mass for the target verb as a function of distance in the strong predictability conditions. 

<<probexpectationweakeninge1,echo=FALSE>>=
## load sentence completion data:
d<-read.table('Pretests/entropy/sentcomp.txt', header=T)

## Expt 1 analysis
exp1<-subset(d, Pre.test==1)
## Isolate a,b conditions:
exp1ab<-subset(exp1,cond%in%c("a","b"))
exp1ab$cond<-factor(exp1ab$cond)
#summary(exp1ab)

target_verbs_ab<-c("kardan","zadan",
                "kardan","kardan",
                "kardan","kardan",
                "kardan","dashtan",
                "bordan","kardan",
                "zadan","zadan",
                "zadan","zadan",
                "zadan","zadan",
                "kardan","kardan",
                "kardan","kardan",
                "dadan","kardan",
                "dadan","dadan",
                "goftan","kardan",
                "kardan","kardan",
                "gozashtan","zadan",
                "kardan","kardan",
                "kardan","kardan",
                "gereftan","kardan")

targets<-data.frame(item=rep(1:36,each=2),
                    cond=rep(letters[1:2],36),
                    target_verbs=rep(target_verbs_ab,each=2))

nrows<-dim(exp1ab)[1]

## Next, create a column marking "correct completion"
hit<-rep(NA,nrows)

## identify exact matches with target:
for(i in 1:nrows){
  ## get i-th row:
  tmp<-exp1ab[i,]
  itemid<-tmp$item.ID
  condition<-as.character(tmp$cond)
  tmpverb<-as.character(tmp$verb)
  ## find appropriate row in targets data frame:
  target_row<-subset(targets,item==itemid & 
                       cond==condition)
  targetverb<-as.character(target_row$target_verbs)
  if(targetverb==tmpverb){
    hit[i]<-1} else {hit[i]<-0}
}

## 1 if target verb produced, 0 otherwise
exp1ab$target<-hit

exp1ab$condition<-ifelse(exp1ab$cond=="b",1,-1)
exp1ab$subj.ID<-factor(exp1ab$subj.ID)

## Fix problem by pasting list no. to make subjects unique:
exp1ab$subject<-factor(paste(exp1ab$subj.ID,exp1ab$LatSq.list,sep=""))

if(0){
#target completion (experiment 1, a v b)
mstanhits<-stan_glmer(target~condition+(1+condition|subject)+(condition|item.ID),
                  family=binomial(),
                  prior_intercept=student_t(df=2),
                  prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  exp1ab,
                  algorithm="sampling",
                  adapt_delta=0.9999,
                  chains=4,
                  iter=2000,
                  cores=4)

save(mstanhits,file="mstanhits.Rda")
}

load("mstanhits.Rda")
mstanhits_tab<-summary_stan_model(mstanhits,nfixefs=1)
@

For the first sentence completion study (which had the RC+PP intervener in the long condition), in the long-distance condition, the probability of producing the target verb fell: on the log-odds scale, the mean and 95\% uncertainty interval were
\Sexpr{as.numeric(as.character(mstanhits_tab[2,2]))}
[\Sexpr{as.numeric(as.character(mstanhits_tab[2,3]))},
\Sexpr{as.numeric(as.character(mstanhits_tab[2,4]))}]
and the posterior probability of the reduction being less than 0  was
\Sexpr{round(as.numeric(as.character(mstanhits_tab[2,5])),digits=2)}.
The odds ratio of producing a target verb in the long vs. short condition was 
\Sexpr{round(exp(as.numeric(as.character(mstanhits_tab[2,2]))),digits=2)},
with 95\% uncertainty interval
[\Sexpr{round(exp(as.numeric(as.character(mstanhits_tab[2,3]))),digits=2)},\Sexpr{round(exp(as.numeric(as.character(mstanhits_tab[2,4]))),digits=2)}].
This means that in the long condition, participants are less likely to 
produce the target verb, but since the uncertainty interval for the odds ratio 
includes 1, the reduction in probability of target verb production is possibly unchanged in the short vs long distance conditions. If anything, there is a weakening of the expectation for the target verb, contrary to the sharpened expectation proposal 
of \citet{konieczny2000locality}.

<<probexpectationweakeninge2,echo=FALSE>>=
## no reduction in grammaticality of target in experiment 2:
exp2abo<-read.table("Pretests/entropy/expt2ab.txt",header=TRUE)
#with(exp2abo,tapply(gram,cond,mean))

if(0){
  mstan.target.expt2<-stan_glmer(target~condition+(1+condition|subj.ID)+(condition|item.ID),
                  family=binomial(),
                  prior_intercept=student_t(df=2),
                  prior=student_t(df=2),
                  prior_covariance=decov(regularization=2),
                  exp2abo,
                  algorithm="sampling",
                  adapt_delta=0.9999,                  
                  chains=4,
                  iter=2000,
                  cores=4)

save(mstan.target.expt2,file="mstan.target.expt2.Rda")  
}
load("mstan.target.expt2.Rda")
mstan.target.expt2_tab<-summary_stan_model(mstan.target.expt2,nfixefs=1)  
mne2<-round(as.numeric(as.character(mstan.target.expt2_tab[2,2])),digits=2)
lowere2<-round(as.numeric(as.character(mstan.target.expt2_tab[2,3])),digits=2)
uppere2<-round(as.numeric(as.character(mstan.target.expt2_tab[2,4])),digits=2)
probe2<-round(as.numeric(as.character(mstan.target.expt2_tab[2,5])),digits=2)
oddsratioe2<-exp(mne2)
@

For the second sentence completion study (which had a PP in the long condition), 
in the long-distance condition, the probability of producing the target verb also fell: the logs odds were
\Sexpr{mne2}
[\Sexpr{lowere2},\Sexpr{uppere2}];
and 
the posterior probability of the reduction being less than 0 was 
\Sexpr{round(probe2,digits=2)}.
The odds ratio of producing a target verb in the long vs. short condition was 
\Sexpr{round(exp(mne2),digits=2)},
with 95\% uncertainty intervals 
[\Sexpr{round(exp(lowere2),digits=2)},
\Sexpr{round(exp(uppere2),digits=2)}].
Thus, in the second sentence completion study, there is only weak evidence of a reduction in probability of producing the target verb in the long-distance condition.  

To summarize, our sentence completion data for experiments 1 and 2's strong predictability condition show that increasing distance tends to reduce the proportion of target verbs produced, although the evidence for this reduction is rather weak overall. Our data from Persian therefore seem to go against the suggestion by \citet{konieczny2000locality} that increasing distance leads to narrowing down the prediction to the target verb. 

Caution is needed in interpreting these results based on the sentence completion data. The biggest issue with the sentence completion data is that it was an offline task; it is difficult to argue that offline completion data can inform us about online processes. It would be much more informative to run an online sentence completion study, forcing participants to make quicker decisions about the sentence completions.
Further, most of our findings relating to the sentence completion data are post-hoc and based on exploratory analyses. 
It would also be very informative to carry out sentence completion studies for experiments such as those of \citet{konieczny2000locality,grodner2005consequences,bartek2011search,vasishth2006argument,vasishth2011locality,levy2013expectation} in order to establish whether increasing distance can weaken expectation cross-linguistically. 

\medskip

In future work it may be worth investigating  existing locality effects in English, German, and Hindi from the perspective of forgetting inducing entropy. 
A further possibility worth investigating is whether entropy reduction \citep{hale06} rather than entropy can
explain the locality effects cross-linguistically.  
In our Persian experiments, it is possible that the entropy at the word preceding the verb is
higher than the entropy at the verb, and it is possible that the reduction in entropy is larger in the long-distance condition. Unfortunately, we have no way to test this in the present design, but future studies could compute entropy reduction empirically in the same way that we computed entropy using sentence completion data. Thus, in principle 
it is possible that entropy reduction
could explain locality effects as well. A related issue that would then arise is whether entropy or entropy reduction furnishes a better explanation for locality effects. 

A broader issue that the above discussion raises is, can all intervention effects be explained via an appeal to information-theoretic metrics?  
\citet{levy2008expectation} had pointed out that information-theoretic metrics cannot explain all the 
results relating to intervention effects; he was mainly referring to locality effects, which can only be explained through memory-based accounts. In later work, \citet{vasishth2011locality,levy2013expectation,levy2013syntactic} also find that both memory and expectation-based accounts are needed to explain the range of observed effects. It is because of the inability of information-theoretic metrics to explain locality effects that \citet{levy2008expectation} argued for ``two-factor'' accounts.
If entropy or some other entropy-based measure turns out be an explanation for locality effects, can we argue for a simpler account that only appeals to information-theoretic metrics? A major empirical problem for such a reductionist account would be the large range and variety of intervention effects (see \citealp{EngelmannJaegerVasishth2015} for a review and computational modeling) that can only be explained through memory-based accounts. Other recent results that would be impossible to explain via a reductionist account are the work by \citet{nicenboim2014individual} and \citet{NicenboimEtAlFrontiers2015Capacity}. Thus, a reductionist account that assumes that \textit{all} effects can be explained by what is predicted next
would always falter when it comes to explaining effects that arise not from predictive processes but from retrieval-based processes.

\subsection{Concluding remarks}

In conclusion, as regards the distance manipulation, the evidence from Persian is in favor of working-memory accounts, although forgetting-causing-entropy is also a candidate explanation. There is not much evidence from Persian that strong-predictability conditions cancel locality effects, as Husain and colleagues had suggested.  Interestingly, there is no evidence in these experiments for the prediction of the expectation account regarding the distance manipulation, that increasing argument-verb distance facilitates processing due to increasing conditional probabilities of the upcoming verb. The suggestion in \citep{levy2013syntactic} that ``the verb-medial languages tend to exhibit the general patterns predicted by memory-based theories, whereas verb-final languages tend to exhibit the general patterns predicted by expectation-based theories''  seems to be difficult to maintain (also see \citet{HusainVasishthNarayanan2015}, for locality effects in Hindi).
One implication of our findings from Persian is that locality and expectation effects observed across studies seem to be highly conditional on the language and syntactic construction being considered---broad cross-linguistic generalizations may be difficult to make.

\section*{Disclosure/Conflict-of-Interest Statement}
The authors declare that the research was conducted in the absence of any
commercial or financial relationships that could be construed as a potential
conflict of interest.

\section*{Acknowledgments}
Thanks to Prof.\ Dr.\ Shahla Raghibdoust in Allameh Tabataba'i University who helped the first author to recruit the participants in Iran. We are grateful to Carla Kessler in University of Potsdam for her help in designing the eye-tracking study using Experiment-Builder software. Many thanks to Lena J\"ager who carried out a careful sanity check for the eye-tracking results. We would like to thank the anonymous reviewers of this article, and the audience in the 28th CUNY conference of human sentence processing in University of Southern California for their insightful feedback. 

\paragraph{Funding\textcolon} This work was supported by the IDEALAB program, and the University of Potsdam.
We acknowledge the support of the Deutsche Forschungsgemeinschaft (German Research Foundation) and Open Access Publication Fund of Potsdam University.

\bibliographystyle{frontiersinSCNS_ENG} 
\bibliography{farnooshbib.bib}

\end{document}
